{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4fc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiments.py\n",
    "import argparse, os, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from fairbench.utils.seed import set_seed\n",
    "from fairbench.datasets import load_dataset   # 통합 로더\n",
    "from fairbench.methods import run_method      # 통합 메서드 실행\n",
    "from fairbench.metrics import compute_metrics # accuracy, supIPM, subgroup metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e192779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Subgroup Fairness Bench\")\n",
    "\n",
    "    # Dataset & data options\n",
    "    p.add_argument(\"--dataset\", type=str, required=True,\n",
    "                   choices=[\"toy_manyq\", \"adult\", \"communities\", \"dutch\", \"celebA\"])\n",
    "    p.add_argument(\"--data_dir\", type=str, default=\"data\")\n",
    "    p.add_argument(\"--q\", type=int, default=100, help=\"toy_manyq sensitive count\")\n",
    "\n",
    "    # X–S handling (공통)\n",
    "    p.add_argument(\"--x_sensitive\", type=str, default=\"drop\",\n",
    "                   choices=[\"drop\", \"keep\", \"concat\"],\n",
    "                   help=\"drop: 민감 원본컬럼을 X에서 제거, keep: 유지, concat: S를 X에 붙여 f(x,s)로 학습\")\n",
    "    p.add_argument(\"--sens_keys\", type=str, default=None,\n",
    "                   help=(\"민감 컬럼 리스트 또는 프리셋 키워드. \"\n",
    "                         \"Adult는 None이면 내부 기본 세트 사용, \"\n",
    "                         \"Communities/Dutch는 None이면 각 로더의 'auto' 프리셋 사용. \"\n",
    "                         \"예: 'race_basic' 또는 'sex,age,education_num'\"))\n",
    "    p.add_argument(\"--sens_thresh\", type=float, default=0.5,\n",
    "                   help=\"민감 수치 컬럼 이진화 분위수 임계(0~1)\")\n",
    "\n",
    "    # Dataset-specific optional paths\n",
    "    p.add_argument(\"--dutch_path\", type=str, default=\"data/raw/dutch.csv\",\n",
    "                   help=\"Dutch CSV 경로(기본 data/raw/dutch.csv)\")\n",
    "    p.add_argument(\"--communities_names\", type=str, default=None,\n",
    "                   help=\"communities.names 경로(기본 data_dir/raw/communities.names)\")\n",
    "    p.add_argument(\"--communities_data\", type=str, default=None,\n",
    "                   help=\"communities.data 경로(기본 data_dir/raw/communities.data)\")\n",
    "    p.add_argument(\"--tfds_data_dir\", type=str, default=None,\n",
    "                help=\"TFDS data_dir (없으면 기본 캐시)\")\n",
    "    p.add_argument(\"--celebA_manual_dir\", type=str, default=None,\n",
    "                help=\"CelebA 수동 다운로드 디렉토리(파일들 존재해야 함)\")\n",
    "\n",
    "    # Method selection\n",
    "    p.add_argument(\"--method\", type=str, required=True,\n",
    "                   choices=[\"dr\", \"gerryfair\", \"multicalib\", \"sequential\", \"reduction\"])\n",
    "\n",
    "    # Reduction (fairlearn ExponentiatedGradient) baseline\n",
    "    p.add_argument(\"--red_constraint\", type=str, default=\"DP\",\n",
    "                   choices=[\"DP\", \"EO\"], help=\"DP=DemographicParity, EO=EqualizedOdds\")\n",
    "    p.add_argument(\"--red_eps\", type=float, default=0.02,\n",
    "                   help=\"fairness slack (작을수록 제약 강함)\")\n",
    "    p.add_argument(\"--red_max_iter\", type=int, default=50)\n",
    "    p.add_argument(\"--red_base\", type=str, default=\"logreg\",\n",
    "                   choices=[\"logreg\", \"linear_svm\", \"rf\", \"mlp_clf\", \"mlp_reg\"],)\n",
    "\n",
    "    # Common training hparams (필요시 각 메서드에서 선택 사용)\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--epochs\", type=int, default=300)\n",
    "    p.add_argument(\"--batch_size\", type=int, default=512)\n",
    "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "\n",
    "    # DR (our method)\n",
    "    p.add_argument(\"--lambda_fair\", type=float, default=0.0,\n",
    "                   help=\"DR fairness weight (정확도-공정성 trade-off)\")\n",
    "    p.add_argument(\"--n_low\", type=int, default=100,\n",
    "                help=\"partial subgroup fairness: 최소 서브그룹 크기 임계값\")\n",
    "    # Data options (여기에 추가)\n",
    "    p.add_argument(\"--shrink_smallest_frac\", type=float, default=1.0,\n",
    "                help=\"전체 n 대비 가장 작은 교차 서브그룹을 이 비율*n까지 다운샘플\")\n",
    "    p.add_argument(\"--shrink_seed\", type=int, default=None,\n",
    "                help=\"다운샘플 시드(미지정시 --seed 사용)\")\n",
    "    p.add_argument(\"--n_low_frac\", type=float, default=None,\n",
    "                help=\"partial subgroup: 최소지지 비율 (0~1). 지정 시 n_low보다 우선\")\n",
    "\n",
    "\n",
    "    # GerryFair\n",
    "    p.add_argument(\"--gamma\", type=float, default=0.01,\n",
    "                   help=\"GerryFair gamma (페어니스 강도/허용 위반 수준)\")\n",
    "    p.add_argument(\"--gf_base\", type=str, default=\"logistic\",\n",
    "                   choices=[\"logistic\", \"linear\", \"mlp_clf\", \"mlp_reg\"])\n",
    "    p.add_argument(\"--gf_max_iters\", type=int, default=10)\n",
    "    p.add_argument(\"--gf_C\", type=float, default=50.0)\n",
    "    p.add_argument(\"--gf_fairness\", type=str, default=\"SP\",\n",
    "                   choices=[\"FP\", \"FN\", \"FPR\", \"FNR\", \"SP\"])\n",
    "    p.add_argument('--decision_threshold', type=float, default=0.5)\n",
    "\n",
    "    # Multicalibration\n",
    "    p.add_argument(\"--mc_alpha\", type=float, default=0.1,\n",
    "                   help=\"Multicalibration alpha (캘리브레이션 허용오차)\")\n",
    "    p.add_argument(\"--mc_lambda\", type=float, default=0.1,\n",
    "                   help=\"Multicalibration lambda (학습 스텝/규제 강도)\")\n",
    "    p.add_argument(\"--mc_max_iter\", type=int, default=30)\n",
    "    p.add_argument(\"--mc_randomized\", action=\"store_true\", default=True)\n",
    "    p.add_argument(\"--mc_use_oracle\", action=\"store_true\", default=False)\n",
    "\n",
    "    # Sequential Fairness\n",
    "    p.add_argument(\"--seq_alpha\", type=float, default=0.1,\n",
    "                   help=\"Sequential Fairness의 alpha/step (기존 0.1을 인자로 노출)\")\n",
    "    p.add_argument(\"--seq_max_iter\", type=int, default=50)\n",
    "\n",
    "    # Results/logging\n",
    "    p.add_argument(\"--exp_name\", type=str, default=None)\n",
    "    p.add_argument(\"--save_dir\", type=str, default=\"results\")\n",
    "    # --- logging / UX ---\n",
    "    p.add_argument(\"--log-interval\", type=int, default=50,\n",
    "                        help=\"train step마다 로그 찍는 간격\")\n",
    "    p.add_argument(\"--progress-bar\", action=\"store_true\",\n",
    "                        help=\"tqdm 진행바 사용\")\n",
    "    p.add_argument(\"--logfile\", type=str, default=\"\",\n",
    "                        help=\"로그를 파일에도 기록 (경로 지정)\")\n",
    "    p.add_argument(\"--heartbeat-secs\", type=int, default=0,\n",
    "                        help=\"N초마다 heartbeat 로그 (0이면 끔)\")\n",
    "    p.add_argument(\"--results-csv\", type=str, default=\"results/all_runs.csv\",\n",
    "                   help=\"실험 결과를 누적 저장할 CSV 경로\")\n",
    "\n",
    "    return p.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a759aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & data options\n",
    "dataset = [\"toy_manyq\", \"adult\", \"communities\", \"dutch\", \"celebA\"][1]  # \"communities\"\n",
    "data_dir = [\"../data/\", \"../data/raw/\"][1]\n",
    "q = [2, 3, 4, 5, 20, 50, 100][0]  # toy_manyq sensitive count\n",
    "\n",
    "# X–S handling\n",
    "x_sensitive = [\"drop\", \"keep\", \"concat\"][0]\n",
    "sens_keys = [\"race_basic\", \"sex,age,education_num\", None][0]\n",
    "sens_thresh = [0.25, 0.5, 0.75][1]\n",
    "\n",
    "# Dataset-specific paths\n",
    "dutch_path = [\"data/raw/dutch.csv\", \"data/dutch_alt.csv\"][0]\n",
    "# communities_names = [\"data/raw/communities.names\", None][0]\n",
    "# communities_data = [\"data/raw/communities.data\", None][0]\n",
    "tfds_data_dir = [None, \"data/tfds\"][0]\n",
    "celebA_manual_dir = [None, \"data/raw/celebA\"][0]\n",
    "\n",
    "# Method selection\n",
    "method = [\"dr\", \"gerryfair\", \"multicalib\", \"sequential\", \"reduction\"][0]\n",
    "\n",
    "# Reduction baseline\n",
    "red_constraint = [\"DP\", \"EO\"][0]\n",
    "red_eps = [0.01, 0.02, 0.05][1]\n",
    "red_max_iter = [10, 50, 100][1]\n",
    "red_base = [\"logreg\", \"linear_svm\", \"rf\", \"mlp_clf\", \"mlp_reg\"][0]\n",
    "\n",
    "# Common training hparams\n",
    "seed = [0, 42, 123][1]\n",
    "epochs = [100, 200, 300][2]\n",
    "batch_size = [128, 256, 512][2]\n",
    "lr = [1e-2, 1e-3, 1e-4][1]\n",
    "\n",
    "# DR (ours)\n",
    "lambda_fair = [0.0, 0.1, 1.0][0]\n",
    "n_low = [50, 100, 200][1]\n",
    "shrink_smallest_frac = [0.5, 1.0][1]\n",
    "shrink_seed = [None, 42][0]\n",
    "n_low_frac = [None, 0.01, 0.05][0]\n",
    "\n",
    "# GerryFair\n",
    "gamma = [0.001, 0.01, 0.1][1]\n",
    "gf_base = [\"logistic\", \"linear\", \"mlp_clf\", \"mlp_reg\"][0]\n",
    "gf_max_iters = [5, 10, 20][1]\n",
    "gf_C = [10.0, 50.0, 100.0][1]\n",
    "gf_fairness = [\"FP\", \"FN\", \"FPR\", \"FNR\", \"SP\"][4]\n",
    "decision_threshold = [0.3, 0.5, 0.7][1]\n",
    "\n",
    "# Multicalibration\n",
    "mc_alpha = [0.05, 0.1, 0.2][1]\n",
    "mc_lambda = [0.05, 0.1, 0.2][1]\n",
    "mc_max_iter = [10, 30, 50][1]\n",
    "mc_randomized = [True, False][0]\n",
    "mc_use_oracle = [True, False][1]\n",
    "\n",
    "# Sequential Fairness\n",
    "seq_alpha = [0.05, 0.1, 0.2][1]\n",
    "seq_max_iter = [10, 30, 50][1]\n",
    "\n",
    "# Results/logging\n",
    "exp_name = [\"exp1\", \"exp2\", None][2]\n",
    "save_dir = [\"../results\", \"outputs\"][0]\n",
    "log_interval = [10, 50, 100][1]\n",
    "progress_bar = [True, False][0]\n",
    "logfile = [\"\", \"logs/run1.log\"][0]\n",
    "heartbeat_secs = [0, 30][0]\n",
    "results_csv = [\"../results/all_runs.csv\", \"../results/exp.csv\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9d264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Dataset & data options\n",
    "args.dataset = dataset\n",
    "args.data_dir = data_dir\n",
    "args.q = q\n",
    "\n",
    "# X–S handling\n",
    "args.x_sensitive = x_sensitive\n",
    "args.sens_keys = sens_keys\n",
    "args.sens_thresh = sens_thresh\n",
    "\n",
    "# Dataset-specific paths\n",
    "args.dutch_path = dutch_path\n",
    "# args.communities_names = communities_names\n",
    "# args.communities_data = communities_data\n",
    "args.tfds_data_dir = tfds_data_dir\n",
    "args.celebA_manual_dir = celebA_manual_dir\n",
    "\n",
    "# Method selection\n",
    "args.method = method\n",
    "\n",
    "# Reduction baseline\n",
    "args.red_constraint = red_constraint\n",
    "args.red_eps = red_eps\n",
    "args.red_max_iter = red_max_iter\n",
    "args.red_base = red_base\n",
    "\n",
    "# Common training hparams\n",
    "args.seed = seed\n",
    "args.epochs = epochs\n",
    "args.batch_size = batch_size\n",
    "args.lr = lr\n",
    "\n",
    "# DR (ours)\n",
    "args.lambda_fair = lambda_fair\n",
    "args.n_low = n_low\n",
    "args.shrink_smallest_frac = shrink_smallest_frac\n",
    "args.shrink_seed = shrink_seed\n",
    "args.n_low_frac = n_low_frac\n",
    "\n",
    "# GerryFair\n",
    "args.gamma = gamma\n",
    "args.gf_base = gf_base\n",
    "args.gf_max_iters = gf_max_iters\n",
    "args.gf_C = gf_C\n",
    "args.gf_fairness = gf_fairness\n",
    "args.decision_threshold = decision_threshold\n",
    "\n",
    "# Multicalibration\n",
    "args.mc_alpha = mc_alpha\n",
    "args.mc_lambda = mc_lambda\n",
    "args.mc_max_iter = mc_max_iter\n",
    "args.mc_randomized = mc_randomized\n",
    "args.mc_use_oracle = mc_use_oracle\n",
    "\n",
    "# Sequential Fairness\n",
    "args.seq_alpha = seq_alpha\n",
    "args.seq_max_iter = seq_max_iter\n",
    "\n",
    "# Results/logging\n",
    "args.exp_name = exp_name\n",
    "args.save_dir = save_dir\n",
    "args.log_interval = log_interval\n",
    "args.progress_bar = progress_bar\n",
    "args.logfile = logfile\n",
    "args.heartbeat_secs = heartbeat_secs\n",
    "args.results_csv = results_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b8bbcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import fairbench.datasets\n",
    "\n",
    "importlib.reload(fairbench)\n",
    "\n",
    "import fairbench.datasets\n",
    "from fairbench.utils.seed import set_seed\n",
    "from fairbench.datasets import load_dataset   # 통합 로더\n",
    "from fairbench.methods import run_method      # 통합 메서드 실행\n",
    "from fairbench.metrics import compute_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962464c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:01:24] args:\n",
      "{\n",
      "  \"dataset\": \"adult\",\n",
      "  \"data_dir\": \"../data/raw/\",\n",
      "  \"q\": 2,\n",
      "  \"x_sensitive\": \"drop\",\n",
      "  \"sens_keys\": \"race_basic\",\n",
      "  \"sens_thresh\": 0.5,\n",
      "  \"dutch_path\": \"data/raw/dutch.csv\",\n",
      "  \"tfds_data_dir\": null,\n",
      "  \"celebA_manual_dir\": null,\n",
      "  \"method\": \"dr\",\n",
      "  \"red_constraint\": \"DP\",\n",
      "  \"red_eps\": 0.02,\n",
      "  \"red_max_iter\": 50,\n",
      "  \"red_base\": \"logreg\",\n",
      "  \"seed\": 42,\n",
      "  \"epochs\": 300,\n",
      "  \"batch_size\": 512,\n",
      "  \"lr\": 0.001,\n",
      "  \"lambda_fair\": 0.0,\n",
      "  \"n_low\": 100,\n",
      "  \"shrink_smallest_frac\": 1.0,\n",
      "  \"shrink_seed\": null,\n",
      "  \"n_low_frac\": null,\n",
      "  \"gamma\": 0.01,\n",
      "  \"gf_base\": \"logistic\",\n",
      "  \"gf_max_iters\": 10,\n",
      "  \"gf_C\": 50.0,\n",
      "  \"gf_fairness\": \"SP\",\n",
      "  \"decision_threshold\": 0.5,\n",
      "  \"mc_alpha\": 0.1,\n",
      "  \"mc_lambda\": 0.1,\n",
      "  \"mc_max_iter\": 30,\n",
      "  \"mc_randomized\": true,\n",
      "  \"mc_use_oracle\": false,\n",
      "  \"seq_alpha\": 0.1,\n",
      "  \"seq_max_iter\": 30,\n",
      "  \"exp_name\": null,\n",
      "  \"save_dir\": \"../results\",\n",
      "  \"log_interval\": 50,\n",
      "  \"progress_bar\": true,\n",
      "  \"logfile\": \"\",\n",
      "  \"heartbeat_secs\": 0,\n",
      "  \"results_csv\": \"../results/all_runs.csv\"\n",
      "}\n",
      "[15:01:24] ▶ prepare_data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sens_keys] tokens=['race_basic']\n",
      "[sens_keys] selected=['sex_Male', 'race_White', 'age_ge_40', 'married', 'edu_bachelor_plus', 'native_US', 'work_gov', 'work_private', 'occ_white_collar', 'occ_blue_collar', 'hours_ge_50', 'cap_gain_pos', 'cap_loss_pos', 'rel_husband']\n",
      "[adult] S columns after filter = ['sex_Male', 'race_White', 'age_ge_40', 'married', 'edu_bachelor_plus', 'native_US', 'work_gov', 'work_private', 'occ_white_collar', 'occ_blue_collar', 'hours_ge_50', 'cap_gain_pos', 'cap_loss_pos', 'rel_husband']\n",
      "[adult] final S_train cols = ['sex_Male', 'race_White', 'age_ge_40', 'married', 'edu_bachelor_plus', 'native_US', 'work_gov', 'work_private', 'occ_white_collar', 'occ_blue_collar', 'hours_ge_50', 'cap_gain_pos', 'cap_loss_pos', 'rel_husband']\n",
      "[S|all] n=48842 cols=['sex_Male', 'race_White', 'age_ge_40', 'married', 'edu_bachelor_plus', 'native_US', 'work_gov', 'work_private', 'occ_white_collar', 'occ_blue_collar', 'hours_ge_50', 'cap_gain_pos', 'cap_loss_pos', 'rel_husband'] (aggregate across splits, ALL 2^k, k=14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:01:26] ✔ prepare_data done in 2.23s\n",
      "[15:01:26] [data] prepared\n",
      "[15:01:26] ▶ run_method:dr ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S] Saved subgroup INTERSECTION stats to CSV: datasets/sensitive_stats_adult.csv\n",
      "  split                                           subgroup  count  proportion  \\\n",
      "0   all  not sex Male & not race White & not age ge 40 ...     21    0.000430   \n",
      "1   all  not sex Male & not race White & not age ge 40 ...      0    0.000000   \n",
      "2   all  not sex Male & not race White & not age ge 40 ...      3    0.000061   \n",
      "3   all  not sex Male & not race White & not age ge 40 ...      0    0.000000   \n",
      "4   all  not sex Male & not race White & not age ge 40 ...      0    0.000000   \n",
      "\n",
      "       n                                            columns  \n",
      "0  48842  sex_Male;race_White;age_ge_40;married;edu_bach...  \n",
      "1  48842  sex_Male;race_White;age_ge_40;married;edu_bach...  \n",
      "2  48842  sex_Male;race_White;age_ge_40;married;edu_bach...  \n",
      "3  48842  sex_Male;race_White;age_ge_40;married;edu_bach...  \n",
      "4  48842  sex_Male;race_White;age_ge_40;married;edu_bach...  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0638c5cb01804fa682652c28571bc70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:01:29] ✔ run_method:dr done in 2.46s\n",
      "[15:01:29] [done] run_method finished\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "# --- logger / heartbeat setup ---\n",
    "from fairbench.utils.logging_utils import setup_logger, Timer, Heartbeat\n",
    "log = setup_logger(\"fair\", logfile or None)\n",
    "hb = Heartbeat(log, heartbeat_secs)\n",
    "hb.start()\n",
    "log.info(\"args:\\n\" + json.dumps(vars(args), indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "exp_name = exp_name or f\"{dataset}_{method}_seed{seed}_{int(time.time())}\"\n",
    "save_dir = Path(save_dir); save_dir.mkdir(exist_ok=True, parents=True)\n",
    "out_csv = save_dir / \"all_results.csv\"\n",
    "\n",
    "# ---- 타이밍 측정 시작 ----\n",
    "job_start_iso = datetime.now().isoformat(timespec=\"seconds\")\n",
    "t0_total = time.perf_counter()\n",
    "prep_sec = np.nan\n",
    "run_sec = np.nan\n",
    "metrics_sec = np.nan\n",
    "\n",
    "# 1) 데이터 로딩 + 2) 방법 실행\n",
    "try:\n",
    "    t0 = time.perf_counter()\n",
    "    with Timer(log, \"prepare_data\"):\n",
    "        data = load_dataset(args)\n",
    "    prep_sec = time.perf_counter() - t0\n",
    "    log.info(\"[data] prepared\")\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    with Timer(log, f\"run_method:{method}\"):\n",
    "        pred_pack = run_method(args, data)\n",
    "    run_sec = time.perf_counter() - t1\n",
    "    log.info(\"[done] run_method finished\")\n",
    "finally:\n",
    "    hb.stop()\n",
    "\n",
    "# 3) 측정\n",
    "t2 = time.perf_counter()\n",
    "report = compute_metrics(args, data, pred_pack)\n",
    "metrics_sec = time.perf_counter() - t2\n",
    "\n",
    "# ---- 타이밍/메타 정보 구성 ----\n",
    "total_sec = time.perf_counter() - t0_total\n",
    "time_info = {\n",
    "    \"start_time\": job_start_iso,\n",
    "    \"end_time\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"time_prepare_data_sec\": round(float(prep_sec), 4),\n",
    "    \"time_run_method_sec\":   round(float(run_sec), 4),\n",
    "    \"time_metrics_sec\":      round(float(metrics_sec), 4),\n",
    "    \"time_total_sec\":        round(float(total_sec), 4),\n",
    "}\n",
    "if isinstance(pred_pack, dict) and (\"note\" in pred_pack):\n",
    "    time_info[\"method_note\"] = str(pred_pack[\"note\"])\n",
    "\n",
    "# data 로더가 meta를 담아줬다면 같이 기록(선택)\n",
    "meta = (data.get(\"meta\", {}) if isinstance(data, dict) else {}) or {}\n",
    "\n",
    "def _as_str_list(x):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return \",\".join(map(str, x))\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "# 실험/페어니스 관련 하이퍼파라미터 모으기\n",
    "hparams = {\n",
    "    # 공통/식별\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"exp_name\": exp_name,\n",
    "    \"dataset\": dataset,\n",
    "    \"method\": method,\n",
    "    \"seed\": seed,\n",
    "\n",
    "    # 데이터/민감속성 설정\n",
    "    \"x_sensitive\": x_sensitive,\n",
    "    \"sens_keys\": sens_keys,\n",
    "    \"sens_thresh\": sens_thresh,\n",
    "\n",
    "    # 학습 공통\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"lr\": lr,\n",
    "\n",
    "    # DR (ours)\n",
    "    \"lambda_fair\": getattr(args, \"lambda_fair\", None),\n",
    "    \"n_low\": getattr(args, \"n_low\", None),\n",
    "    \"lambda_fair\": getattr(args, \"lambda_fair\", None),\n",
    "    \"n_low\": getattr(args, \"n_low\", None),\n",
    "    \"n_low_frac\": getattr(args, \"n_low_frac\", None),   # NEW\n",
    "    \"shrink_smallest_frac\": getattr(args, \"shrink_smallest_frac\", None),  # NEW\n",
    "    \"shrink_seed\": getattr(args, \"shrink_seed\", None),\n",
    "\n",
    "    # GerryFair\n",
    "    \"gamma\": getattr(args, \"gamma\", None),\n",
    "    \"gf_max_iters\": getattr(args, \"gf_max_iters\", None),\n",
    "    \"gf_C\": getattr(args, \"gf_C\", None),\n",
    "    \"gf_fairness\": getattr(args, \"gf_fairness\", None),\n",
    "\n",
    "    # Multicalibration\n",
    "    \"mc_alpha\": getattr(args, \"mc_alpha\", None),\n",
    "    \"mc_lambda\": getattr(args, \"mc_lambda\", None),\n",
    "    \"mc_max_iter\": getattr(args, \"mc_max_iter\", None),\n",
    "    \"mc_randomized\": getattr(args, \"mc_randomized\", None),\n",
    "    \"mc_use_oracle\": getattr(args, \"mc_use_oracle\", None),\n",
    "\n",
    "    # Sequential\n",
    "    \"seq_alpha\": getattr(args, \"seq_alpha\", None),\n",
    "    \"seq_max_iter\": getattr(args, \"seq_max_iter\", None),\n",
    "\n",
    "    # Reduction (fairlearn ExponentiatedGradient)\n",
    "    \"red_constraint\": getattr(args, \"red_constraint\", None),\n",
    "    \"red_eps\": getattr(args, \"red_eps\", None),\n",
    "    \"red_max_iter\": getattr(args, \"red_max_iter\", None),\n",
    "    \"red_base\": getattr(args, \"red_base\", None),\n",
    "\n",
    "    # 로더 메타(있으면 기록)\n",
    "    \"x_sensitive_mode\": meta.get(\"x_sensitive_mode\"),\n",
    "    \"used_S_cols\": _as_str_list(meta.get(\"used_S_cols\")),\n",
    "    \"dropped_cols\": _as_str_list(meta.get(\"dropped_cols\")),\n",
    "}\n",
    "# 타이밍 정보 합치기\n",
    "hparams.update(time_info)\n",
    "\n",
    "# 한 행(row)으로 합치기: report가 동일 키를 갖고 있으면 report 값을 우선\n",
    "row = {**hparams, **report}\n",
    "\n",
    "# 4) 저장\n",
    "Path(out_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "df = pd.DataFrame([row])\n",
    "df.to_csv(out_csv, mode=\"a\", header=not os.path.exists(out_csv), index=False)\n",
    "print(f\"[SAVE+APPEND] {out_csv} (+1 row)\")\n",
    "print(json.dumps(report, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ec603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(args, data, pred_pack):\n",
    "proba = pred_pack.get(\"proba\", None)\n",
    "pred  = pred_pack.get(\"pred\",  None)\n",
    "\n",
    "report = dict(dataset=getattr(args, \"dataset\", \"\"),\n",
    "                method=getattr(args, \"method\", \"\"),\n",
    "                seed=getattr(args, \"seed\", None))\n",
    "\n",
    "thr = getattr(args, \"thr\", 0.5)\n",
    "min_support = getattr(args, \"min_support\", 5)\n",
    "mc_bins = getattr(args, \"mc_bins\", 10)\n",
    "mc_min_support = getattr(args, \"mc_min_support\", 10)\n",
    "\n",
    "# --- 정답 y 수집 ---\n",
    "if data[\"type\"] == \"tabular\":\n",
    "    y = data.get(\"y_test\", None)\n",
    "    S = data.get(\"S_test\", None)\n",
    "else:\n",
    "    ys, Ss = [], []\n",
    "    for _, yb, Sb in data[\"test_loader\"]:\n",
    "        ys.append(yb.numpy())\n",
    "        Ss += Sb\n",
    "    y = np.concatenate(ys) if len(ys) > 0 else None\n",
    "    S = Ss  # 이미지: list[dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666208a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47991</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46304</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10268</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30513</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7330</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9077</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55575</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12318</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43502</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46155</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12084 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sex_bin\n",
       "47991        0\n",
       "46304        0\n",
       "10268        0\n",
       "30513        0\n",
       "7330         0\n",
       "...        ...\n",
       "9077         0\n",
       "55575        0\n",
       "12318        0\n",
       "43502        0\n",
       "46155        0\n",
       "\n",
       "[12084 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f95770",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'HKRR'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairbench\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticalibration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticalibrationPredictor\n",
      "File \u001b[0;32m~/2025-09-09-subgroup/main/fairbench/methods/multicalibration/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmcb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticalibrationPredictor\n",
      "File \u001b[0;32m~/2025-09-09-subgroup/main/fairbench/methods/multicalibration/mcb.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mHKRR\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhkrr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HKRRAlgorithm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mHKRR\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhkrr_multiclass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticlassHKRRAlgorithm\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mHJZ\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhjz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HJZAlgorithm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'HKRR'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af162a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eec089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548b280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(args, \"seq_alpha\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aacc7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from fairbench.utils.trainval import split_train_val\n",
    "import numpy as np\n",
    "\n",
    "n = 20000; d = 20; q = 2\n",
    "assert d > q, f\"d must be larger than q, but got d={d}, q={q}\"\n",
    "rng = np.random.default_rng(args.seed)\n",
    "X = rng.normal(size=(n, d)).astype(np.float32)\n",
    "\n",
    "# latent sensitive factors (q binary), correlated with some features\n",
    "S = (rng.normal(size=(n, q)) + 0.5*X[:, :min(d, q)][:, None]).mean(axis=1, keepdims=True)\n",
    "S = (rng.normal(size=(n, q)) > 0.0).astype(int)  # 독립적인 q도 가능\n",
    "S = pd.DataFrame(S, columns=[f\"s{j}\" for j in range(q)])\n",
    "\n",
    "# target: non-linear function + mild spurious corr with a few sensitive attrs\n",
    "logits = 0.8*X[:,0] - 0.6*X[:,1] + 0.5*X[:,2]*X[:,3] + 0.2*(S.values[:,:min(q,5)].sum(axis=1))\n",
    "p = 1/(1+np.exp(-logits))\n",
    "y = (rng.uniform(size=n) < p).astype(int)\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f\"x{j}\" for j in range(d)])\n",
    "\n",
    "X_tr, X_te, y_tr, y_te, S_tr, S_te = train_test_split(X, y, S, test_size=0.2, random_state=args.seed, stratify=y)\n",
    "X_tr, X_va, y_tr, y_va, S_tr, S_va = split_train_val(X_tr, y_tr, S_tr, val_size=0.2, seed=args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2980dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s0</th>\n",
       "      <th>s1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       s0  s1\n",
       "0       0   1\n",
       "1       0   1\n",
       "2       0   0\n",
       "3       1   0\n",
       "4       1   1\n",
       "...    ..  ..\n",
       "19995   1   1\n",
       "19996   0   1\n",
       "19997   0   1\n",
       "19998   1   0\n",
       "19999   1   1\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12808382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.13617472, -0.5246417 ]],\n",
       "\n",
       "       [[-0.108615  , -0.34511442]],\n",
       "\n",
       "       [[ 0.35544327,  0.26692747]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.04899138,  0.0732359 ]],\n",
       "\n",
       "       [[-0.95700005, -0.67619325]],\n",
       "\n",
       "       [[ 0.33115828,  0.19019868]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n = 20000; d = 20; q = 2\n",
    "assert d > q, f\"d must be larger than q, but got d={d}, q={q}\"\n",
    "rng = np.random.default_rng(args.seed)\n",
    "X = rng.normal(size=(n, d)).astype(np.float32)\n",
    "\n",
    "(rng.normal(size=(n, q)) + 0.5*X[:, :min(d, q)][:, None]).mean(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d976eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"results_0909/dr_subgroup_subset_3q_adult/all_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c1aeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'exp_name', 'dataset', 'method', 'seed', 'x_sensitive',\n",
       "       'sens_keys', 'sens_thresh', 'epochs', 'batch_size', 'lr', 'lambda_fair',\n",
       "       'n_low', 'n_low_frac', 'shrink_smallest_frac', 'shrink_seed', 'gamma',\n",
       "       'gf_max_iters', 'gf_C', 'gf_fairness', 'mc_alpha', 'mc_lambda',\n",
       "       'mc_max_iter', 'mc_randomized', 'mc_use_oracle', 'seq_alpha',\n",
       "       'seq_max_iter', 'red_constraint', 'red_eps', 'red_max_iter', 'red_base',\n",
       "       'x_sensitive_mode', 'used_S_cols', 'dropped_cols', 'start_time',\n",
       "       'end_time', 'time_prepare_data_sec', 'time_run_method_sec',\n",
       "       'time_metrics_sec', 'time_total_sec', 'V_stats', 'accuracy',\n",
       "       'supipm_rbf', 'supipm_w1', 'sup_mmd_dfcols', 'sup_w1_dfcols',\n",
       "       'sup_mmd_over_V', 'sup_w1_over_V', 'spd_worst', 'spd_mean',\n",
       "       'worst_weighted_group_spd', 'mean_weighted_group_spd',\n",
       "       'worst_spd_over_V', 'mean_spd_over_V', 'worst_weighted_spd_over_V',\n",
       "       'mean_weighted_spd_over_V', 'fpr_worst', 'fpr_mean', 'mc_worst',\n",
       "       'mc_mean', 'marg_spd_worst', 'marg_spd_mean', 'marg_fpr_worst',\n",
       "       'marg_fpr_mean', 'marg_mc_worst', 'marg_mc_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
