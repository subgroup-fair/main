# fairbench/methods/dr.py
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import StandardScaler
from ..utils.threshold import tune_threshold
import logging, time
from tqdm.auto import tqdm
from fairbench.utils.logging_utils import Timer, mem_str
from itertools import product, combinations

log = logging.getLogger("fair")

# ----- ëª¨ë¸ë“¤ -----
class MLP(nn.Module):
    def __init__(self, d):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d, 128), nn.ReLU(), nn.Dropout(0.1),
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, 1)   # logits
        )
    def forward(self, x): return self.net(x).squeeze(-1)

class Linear(nn.Module):
    def __init__(self, d):
        super().__init__()
        self.net = nn.Linear(d, 1)
    def forward(self, x): return self.net(x).squeeze(-1)

class SmallConvNet(nn.Module):
    """CelebAìš© ê²½ëŸ‰ CNN (SequentialFairnessë¥˜ì˜ ë‹¨ìˆœ ConvNet ê¸°ì¡°)"""
    def __init__(self, n_classes=1):
        super().__init__()
        ch=[32,64,128,128]
        def block(cin, cout):
            return nn.Sequential(
                nn.Conv2d(cin, cout, 3, padding=1, bias=False),
                nn.BatchNorm2d(cout),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2)
            )
        self.features = nn.Sequential(
            block(3, ch[0]),
            block(ch[0], ch[1]),
            block(ch[1], ch[2]),
            block(ch[2], ch[3]),
        )
        self.head = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(ch[3], n_classes))
    def forward(self, x): return self.head(self.features(x)).squeeze(-1)

# ----- DR ì„œë¸Œë£¨í‹´ -----
class Discriminator(nn.Module):
    """g(z): scalar"""
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(1,32), nn.ReLU(), nn.Linear(32,16), nn.ReLU(), nn.Linear(16,1))
    def forward(self, z): return self.net(z)

def artanh_corr_old(yv, gz, eps=1e-6):
    y_c = yv - yv.mean()
    g_c = gz - gz.mean()
    y_std = torch.sqrt((y_c**2).mean() + eps); g_std = torch.sqrt((g_c**2).mean() + eps)
    corr = (y_c*g_c).mean()/(y_std*g_std + eps)
    corr_abs = torch.clamp(torch.abs(corr), 0.0, 1.0-1e-6)
    return torch.atanh(corr_abs)

def artanh_corr(yv, gz, eps=1e-6):
    print("new r^2")
    ## y_c = v^t*c, g_c = g(f(x,s))
    y_c = yv - yv.mean()
    g_c = gz - gz.mean()

    y_std = torch.sqrt((y_c**2).mean() + eps)
    # g_std = torch.sqrt((g_c**2).mean() + eps)

    rr = (y_c*g_c).mean()/(y_std + eps)
    corr_abs = torch.clamp(torch.abs(rr/2), 0.0, 1.0-1e-6)

    return torch.atanh(corr_abs)



def _build_V_3style_from_S_df(S_df, min_support=1):
    """
    3^q ìŠ¤íƒ€ì¼(ì¼ë°˜í™”: ê° familyì—ì„œ {ALL} âˆª {ê° ì¹´í…Œê³ ë¦¬==1} âˆª (ì´ì§„ì´ë©´ {col==0}) ì¤‘ í•˜ë‚˜ ì„ íƒ)
    â†’ ëª¨ë“  family ì„ íƒì„ ANDë¡œ ê²°í•© â†’ ğ’± ìƒì„±.
    - ê³µì§‘í•©(ì „ë¶€ ALL)ì€ ì œì™¸
    - min_support ë¯¸ë§Œì€ í”„ë£¨ë‹
    ë°˜í™˜:
      V: bool mask ë¦¬ìŠ¤íŠ¸
      choices: ê° V[k]ì— ëŒ€í•œ per-family ì„ íƒ(spec) ë¦¬ìŠ¤íŠ¸(ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆë„ë¡ ê¸°ë¡)
    """
    n = len(S_df)
    if n == 0:
        return [], []

    # family ë¬¶ê¸°
    fam = {}
    for c in S_df.columns:
        parts = str(c).split("_", 1)
        f = parts[0] if len(parts) > 1 else str(c)
        fam.setdefault(f, []).append(c)

    arrays = {c: S_df[c].values.astype(int) for c in S_df.columns}

    # familyë³„ ì˜µì…˜ ì„¸íŠ¸ êµ¬ì„±
    # - í•­ìƒ ('ALL', None)
    # - ì›í•« multi-cat: ê° ('col1', col)
    # - ì´ì§„(ì»¬ëŸ¼ 1ê°œ): ('col1', col) + ('col0', col)
    fam_opts = []
    fam_names = []
    for f, cols in fam.items():
        opts = [("ALL", None)]
        for col in cols:
            opts.append(("col1", col))
        if len(cols) == 1:
            col = cols[0]
            opts.append(("col0", col))
        fam_opts.append(opts)
        fam_names.append(f)

    # ì „ì²´ ì¡°í•© ìˆ˜ = âˆ_family |opts|
    total = 1
    for opts in fam_opts:
        total *= len(opts)

    V, choices = [], []
    seen = set()
    with tqdm(total=total, desc=f"[DR-V] build 3^q-style", leave=False) as pbar:
        for pick in product(*fam_opts):
            pbar.update(1)
            # ì „ë¶€ ALL(= ì „ì²´ ë°ì´í„°) ì„ íƒì€ ìŠ¤í‚µ
            if all(tag == "ALL" for tag, _ in pick):
                continue

            m = np.ones(n, dtype=bool)
            ok = True
            for (tag, col) in pick:
                if tag == "ALL":
                    continue
                colv = arrays[col]
                if tag == "col1":
                    m &= (colv == 1)
                else:  # 'col0'
                    m &= (colv == 0)
                if not m.any():
                    ok = False
                    break
            if not ok:
                continue

            ns = int(m.sum())
            if ns < int(min_support):
                continue

            key = m.tobytes()
            if key in seen:
                continue
            seen.add(key)

            # ì„ íƒ ì‚¬ì–‘ì„ ì‚¬ëŒì´ ì½ê¸° ì‰½ê²Œ ê¸°ë¡
            readable = []
            for (fam_name, (tag, col)) in zip(fam_names, pick):
                if tag == "ALL":
                    readable.append(f"{fam_name}=ALL")
                elif tag == "col1":
                    readable.append(f"{fam_name}=={col}")
                else:
                    readable.append(f"{fam_name}==NOT({col})")  # binary complement
            V.append(m)
            choices.append(tuple(readable))

    return V, choices


def _build_V_and_C_from_S_df(S_df, device="cpu", n_low=None, n_low_frac=None):
    """
    S_df â†’ ì›ì â†’ ëª¨ë“  í•©ì§‘í•© V (min_support í”„ë£¨ë‹) â†’ C (N x |V|)
    ê° ì—´ì€ c_{im} = 1[i âˆˆ G_m]. (ì–‘/ìŒ ìµœì†Œì§€ì§€ ë‘˜ ë‹¤ ë§Œì¡±í•˜ëŠ” ì—´ë§Œ ì‚¬ìš©)
    + V_stats: ê° G_mì˜ ìƒ˜í”Œ ìˆ˜ í†µê³„ ì €ì¥
    """
    N = len(S_df)
    if (n_low_frac is not None) and (n_low_frac > 0):
        min_support = int(np.ceil(float(n_low_frac) * N))
    else:
        min_support = int(n_low or 0)


    print("min_suppoert:", min_support)

    V, choices = _build_V_3style_from_S_df(S_df, min_support=min_support)
    # import pdb; pdb.set_trace()
    log.info(f"[DR-V] 3^q-style |V|={len(V)} (min_support={min_support})")
    if len(V) == 0:
        return [], torch.zeros((N,0), device=device), []

    # ë‘ í¸(ê·¸ë£¹/ë³´ì™„) ëª¨ë‘ ìµœì†Œì§€ì§€ ë§Œì¡±í•˜ëŠ” ì—´ë§Œ ì„ íƒ + í†µê³„ ì €ì¥
    cols = []
    V_stats = []   # â˜… ì¶”ê°€: ê° subgroup-subsetë³„ ìƒ˜í”Œ ìˆ˜ ê¸°ë¡
    kept = 0
    for m in V:
        ns = int(m.sum()); nsc = N - ns
        if ns >= min_support and nsc >= min_support:
            cols.append(torch.tensor(m.astype(np.float32), device=device).view(-1,1))
            V_stats.append(ns)
            kept += 1
    # import pdb; pdb.set_trace()
    if kept == 0:
        C = torch.zeros((N,0), device=device)
    else:
        C = torch.cat(cols, dim=1)

    log.info(f"[DR-V] C built with {C.shape[1]} columns (filtered by both-sides support)")
    log.info(f"[DR-V] stats example (first 3): {V_stats[:3] if len(V_stats)>0 else []}")
    return V, C, choices, V_stats



# ----- tabular í•™ìŠµ -----
def train_tabular(args, data, device):
    X_tr = torch.tensor(data["X_train"].values, dtype=torch.float32, device=device)
    y_tr = torch.tensor(data["y_train"], dtype=torch.float32, device=device)
    X_va = torch.tensor(data["X_val"].values, dtype=torch.float32, device=device)
    y_va = torch.tensor(data["y_val"], dtype=torch.float32, device=device)
    S_tr = data["S_train"]; S_va = data["S_val"]

    n, d = X_tr.shape
    # f = MLP(d).to(device)
    f = Linear(d).to(device)
    g = Discriminator().to(device)

    n_low = getattr(args, "n_low", None)
    n_low_frac = getattr(args, "n_low_frac", None)
    print("========================================================")
    print("n_low:", n_low, ", n_low_frac:", n_low_frac)
    print("========================================================")

    # === NEW: subgroup-subset Vì™€ C (N x |V|) êµ¬ì„±
    with torch.no_grad():
        V_tr, Ctr, choices, V_stats = _build_V_and_C_from_S_df(
            S_tr, device=device, n_low=n_low, n_low_frac=n_low_frac
        )

        v = nn.Parameter(torch.randn(Ctr.shape[1], device=device))
        v.data = v / (v.norm() + 1e-12)

    opt_f = optim.Adam(f.parameters(), lr=args.lr, weight_decay=1e-7)
    opt_g = optim.Adam(g.parameters(), lr=args.lr*3)
    opt_v = optim.Adam([v], lr=args.lr*10)

    loss_bce = nn.BCEWithLogitsLoss()

    best_val = float("inf")   # tie-breaker ìš©
    best_acc = -1.0           # â˜… ACC ê¸°ì¤€ ì„ íƒ
    best_f = None             # fì˜ ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ë§Œ ì €ì¥(í…ŒìŠ¤íŠ¸ì—” fë§Œ í•„ìš”)
    thr = float(getattr(args, "thr", 0.5))  # ê³ ì • ì„ê³„ê°’
    adv_k = int(getattr(args, "adv_steps", 3))  # (ì˜µì…˜) adversarial step ì¡°ì ˆ

    for ep in tqdm(range(args.epochs)):
        # ===== train step =====
        f.train(); g.train()

        logit = f(X_tr)                 # [N]
        cls = loss_bce(logit, y_tr)     # BCE(logits, targets)

        if args.lambda_fair == 0.0 or Ctr.shape[1] == 0:
            total = cls
            opt_f.zero_grad(); total.backward(); opt_f.step()
        else:
            with torch.no_grad():
                v_unit = v / (v.norm() + 1e-12)
            yv = (Ctr @ v_unit).detach()               # [N]
            gz = g(logit.unsqueeze(1)).squeeze(-1)     # [N]
            dr = artanh_corr(yv, gz)                   # ìŠ¤ì¹¼ë¼
            total = cls + args.lambda_fair * dr
            opt_f.zero_grad(); total.backward(retain_graph=True); opt_f.step()

            # adversary k-step
            for _ in range(adv_k):
                logit_det = f(X_tr).detach()
                v_unit = v / (v.norm() + 1e-12)
                yv = (Ctr @ v_unit)
                gz = g(logit_det.unsqueeze(1)).squeeze(-1)
                dr = artanh_corr(yv, gz)
                opt_g.zero_grad(); (-dr).backward(retain_graph=True); opt_g.step()
                opt_v.zero_grad(); (-dr).backward(); opt_v.step()
                with torch.no_grad():
                    v.data = v.data / (v.data.norm() + 1e-12)

        # ===== validation step (â˜… ACCë¡œ ì„ íƒ, tieëŠ” BCEë¡œ) =====
        f.eval()
        with torch.no_grad():
            logit_va = f(X_va)
            prob_va = torch.sigmoid(logit_va)
            pred_va = (prob_va >= thr).float()
            val_acc = (pred_va == y_va).float().mean().item()
            val_loss = loss_bce(logit_va, y_va).item()

        if (val_acc > best_acc) or (np.isclose(val_acc, best_acc) and val_loss < best_val):
            best_acc = val_acc
            best_val = val_loss
            best_f = {k: v_.detach().cpu().clone() for k, v_ in f.state_dict().items()}

    # ===== best model ë³µì› & í…ŒìŠ¤íŠ¸ =====
    if best_f is not None:
        f.load_state_dict({k: v_.to(device) for k, v_ in best_f.items()})

    f.eval()
    with torch.no_grad():
        p_test = torch.sigmoid(f(torch.tensor(
            data["X_test"].values, dtype=torch.float32, device=device
        ))).cpu().numpy()
    yhat = (p_test >= thr).astype(int)

    return dict(proba=p_test, pred=yhat, V_stats = V_stats, choices = choices)


def _train_image(args, data, device):
    f = SmallConvNet().to(device)
    g = Discriminator().to(device)
    opt_f = optim.Adam(f.parameters(), lr=args.lr, weight_decay=1e-4)
    opt_g = optim.Adam(g.parameters(), lr=args.lr*3)
    loss_bce = nn.BCEWithLogitsLoss()

    thr = float(getattr(args, "thr", 0.5))  # ê³ ì • ì„ê³„ê°’
    best_val = float("inf")
    best_f = None

    def batch_C(S_list, n_low_frac=None, n_low=None):
        keys = list(data["meta"]["sens_list"])
        N = len(S_list)
        if (n_low_frac is not None) and (n_low_frac > 0):
            min_support = int(np.ceil(float(n_low_frac) * N))
        else:
            min_support = int(n_low or 0)
        Ms = []
        for k in keys:
            m = torch.tensor([s[k] for s in S_list], dtype=torch.float32, device=device).view(-1,1)
            cnt = int(m.sum().item()); neg = N - cnt
            if cnt >= min_support and neg >= min_support:
                Ms.append(m)
        return torch.cat(Ms, dim=1) if len(Ms) > 0 else torch.zeros((N,0), device=device)


    for ep in range(args.epochs):
        # ===== train =====
        f.train(); g.train()
        for x,y,S in data["train_loader"]:
            x=x.to(device); y=y.float().to(device)
            logit = f(x)
            cls = loss_bce(logit, y)

            if args.lambda_fair == 0.0:
                opt_f.zero_grad(); cls.backward(); opt_f.step(); continue

            # Cb = batch_C(S, n_low=16)
            Cb = batch_C(S, n_low_frac=getattr(args, "n_low_frac", None), n_low=getattr(args, "n_low", None))
            if Cb.shape[1] == 0:
                opt_f.zero_grad(); cls.backward(); opt_f.step(); continue

            with torch.no_grad():
                v_ = torch.randn(Cb.shape[1], device=device)
                v_ = v_ / (v_.norm() + 1e-12)

            yv = (Cb @ v_)                        # [B]
            gz = g(logit.unsqueeze(1)).squeeze(-1)
            dr = artanh_corr(yv, gz)

            total = cls + args.lambda_fair * dr
            opt_f.zero_grad(); total.backward(retain_graph=True); opt_f.step()
            opt_g.zero_grad(); (-dr).backward(); opt_g.step()

        # ===== validation (BCEë§Œìœ¼ë¡œ ì„ íƒ) =====
        f.eval()
        val_losses = []
        with torch.no_grad():
            for x,y,S in data["val_loader"]:
                x=x.to(device); y=y.float().to(device)
                logit = f(x)
                val_losses.append(loss_bce(logit, y).item())
        val_loss = float(np.mean(val_losses)) if len(val_losses)>0 else float("inf")

        if val_loss < best_val:
            best_val = val_loss
            best_f = {k: v_.detach().cpu().clone() for k, v_ in f.state_dict().items()}

    # ===== best model ë³µì› & í…ŒìŠ¤íŠ¸ =====
    if best_f is not None:
        f.load_state_dict({k: v_.to(device) for k, v_ in best_f.items()})

    f.eval()
    pt=[]
    with torch.no_grad():
        for x,y,S in data["test_loader"]:
            p = torch.sigmoid(f(x.to(device))).cpu().numpy()
            pt.append(p)
    pt = np.concatenate(pt)
    yhat = (pt >= thr).astype(int)
    return dict(proba=pt, pred=yhat)

def run_dr_subgroup_subset_3q(args, data):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if data["type"] == "tabular":
        print("=== DR-subgroup-subset-3q (tabular) ===")
        print("=== DR-subgroup-subset-3q (tabular) ===")
        print("=== DR-subgroup-subset-3q (tabular) ===")
        return train_tabular(args, data, device)
    elif data["type"] == "image":
        return _train_image(args, data, device)
    else:
        raise ValueError(data["type"])
