# fairbench/metrics/subgroup_measures.py
import numpy as np
from tqdm import tqdm

## ê±´ì›…ì´ê°€ ë„˜ê²¨ì¤€ê±°: SPD/FPR/MC (worst, mean, marginal)
#########################################################
## ìš°ë¦¬ê°€ ì‹¤í—˜í•  metrics (subgroup-based)
## [vv] 1) sup_mmd_dfcols: ë‘ ë¶„í¬ì˜ MMD.                                                                  - singleton subgroup
## [vv] 2) sup_w1_dfcols: ë‘ ë¶„í¬ì˜ WD.                                                                     - singleton subgroup
## [vv] 3) sup_mmd_over_V: ì „ì²´ vs ê° subgroup ë¶„í¬ MMDì˜ ìµœëŒ“ê°’.                                           - ğ’± (subgroup subset)
## [vv] 4) sup_w1_over_V: ì „ì²´ vs ê° subgroup ë¶„í¬ WDì˜ ìµœëŒ“ê°’.                                             - ğ’± (subgroup subset)

## [v] 5,6) worst/mean worst_group_spd, mean_group_spd: SPD.                                                - singleton subgroup
## [vv] 7,8) worst/mean worst_weighted_group_spd, mean_weighted_group_spd : ê·¸ë£¹ ë¹ˆë„ë¡œ ê°€ì¤‘ í‰ê· ëœ SPD.          - singleton subgroup
## [v] 9,10) worst/mean worst_spd_over_V,mean_spd_over_V : SPD.                                                - ğ’± (subgroup subset)
## [vv] 11,12) worst/mean worst_weighted_spd_over_V, mean_weighted_spd_over_V: ê·¸ë£¹ ë¹ˆë„ë¡œ ê°€ì¤‘ í‰ê· ëœ SPD.        - ğ’± (subgroup subset)

#########################################################
## ê¸°ì¡´ subgroup ë©”íŠ¸ë¦­ (worst/mean)
## 1) worst/mean subgroup/marginal FPR gap
## 2) worst/mean subgroup/marginal multicalibration gap
#########################################################



# parameter
# min_support: ê·¸ë£¹ì— ì´ê±°ë³´ë‹¤ ìƒ˜í”Œ ìˆ˜ ì ìœ¼ë©´ metric ê³„ì‚° ì•ˆí• ê²Œ
# sigma: MMDì— ìˆëŠ” RBF kernel bandwidth (Noneì´ë©´ ìƒ˜í”Œê°„ ê±°ë¦¬ì˜ ì¤‘ì•™ê°’ì„ ì“°ê² ë‹¤)

def _as_mask_list_from_V(V, n):
    """
    subgroup subset ì´ inputìœ¼ë¡œ ë“¤ì–´ì˜¤ë©´ ê° ìƒ˜í”Œì´ í•´ë‹¹ ssì— ë“¤ì–´ê°€ëŠ”ì§€ ì—¬ë¶€ ë°˜í™˜
    subgroup subset 10ê°œ ë“¤ì–´ì˜¤ë©´ ê° ìƒ˜í”Œì˜ 10ê°œ bool mask ì¶œë ¥
    V: list of group specs. ê° ì›ì†ŒëŠ”
       - bool mask (shape (n,))
       - ì •ìˆ˜ ì¸ë±ìŠ¤ ë°°ì—´
    return: list[np.ndarray(bool, shape (n,))]
    """
    masks = []
    for g in V:
        if isinstance(g, np.ndarray) and g.dtype == bool:
            m = g
        else:
            idx = np.asarray(g, dtype=int).reshape(-1)
            m = np.zeros(n, dtype=bool)
            m[idx] = True
        masks.append(m)
    return masks


def _rate(pred):
    return float(np.mean(pred)) if len(pred)>0 else np.nan


## [ ] 1) mmd_statistic(P,Q): ë‘ ë¶„í¬ì˜ MMD.
## [ ] 3) sup_mmd: ì „ì²´ vs ê° subgroup ë¶„í¬ MMDì˜ ìµœëŒ“ê°’.


def _pairwise_rbf_sum(x, sigma):
    """
    âˆ‘_{i<j} k(x_i,x_j) with RBF. (self-term ì œì™¸)
    E[k(x,x')] ê³„ì‚°í•˜ëŠ”ì• 
    """
    x = x.reshape(-1, 1)
    # (x_i - x_j)^2 = (x^2) + (x^2)^T - 2 x x^T
    x2 = (x ** 2).sum(axis=1, keepdims=True)
    d2 = x2 + x2.T - 2.0 * (x @ x.T)
    K = np.exp(-d2 / (2.0 * sigma * sigma))
    # ëŒ€ê°ì„ 0ìœ¼ë¡œ
    np.fill_diagonal(K, 0.0)
    # ìƒì‚¼ê° í•©
    return float(K.sum() / 2.0)

def _mmd2_unbiased(x, y, sigma):
    """
    Unbiased MMD^2 for RBF kernel (Gretton et al.).
    MMD^2 = E[k(x,x')] + E[k(y,y')] - 2E[k(x,y)]
    """
    x = np.asarray(x).reshape(-1)
    y = np.asarray(y).reshape(-1)
    nx, ny = x.size, y.size
    if nx < 2 or ny < 2:
        return np.nan
    k_xx = _pairwise_rbf_sum(x, sigma)
    k_yy = _pairwise_rbf_sum(y, sigma)
    # êµí•­: ëª¨ë“  (i in x, j in y)
    x_ = x.reshape(-1, 1)
    y_ = y.reshape(1, -1)
    d2 = (x_ ** 2) + (y_ ** 2) - 2.0 * (x_ @ y_)
    k_xy = np.exp(-d2 / (2.0 * sigma * sigma)).sum()
    mmd2 = (2.0 / (nx * (nx - 1))) * k_xx \
         + (2.0 / (ny * (ny - 1))) * k_yy \
         - (2.0 / (nx * ny)) * k_xy
    return float(max(mmd2, 0.0))

def _median_heuristic_sigma(z):
    """
    median heuristic for 1D scores.
    """
    z = np.asarray(z).reshape(-1)
    if z.size < 2:
        return 1.0
    zz = np.sort(z)
    d = np.abs(zz[1:] - zz[:-1])
    med = np.median(d)
    return float(max(med, 1e-3))

## [ ] 1) mmd_statistic(P,Q): ë‘ ë¶„í¬ì˜ MMD.
def sup_mmd_dfcols(proba, S_df, sigma=None, min_support=5):
    """
    ê° S_df[col]ì„ ê·¸ë£¹ìœ¼ë¡œ ë³´ê³  sup MMD(ê·¸ë£¹ vs ë³´ì™„) ê³„ì‚°.
    """
    p = np.asarray(proba).reshape(-1)
    n = p.size
    if sigma is None:
        sigma = _median_heuristic_sigma(p)
    best = 0.0
    for c in S_df.columns:
        m = S_df[c].values.astype(int) == 1
        ns, nsc = int(m.sum()), int((~m).sum())
        if ns < min_support or nsc < min_support:
            continue
        v = _mmd2_unbiased(p[m], p[~m], sigma) ** 0.5  # MMD (not squared)
        if v > best:
            best = v
    return float(best)

## [ ] 3) sup_mmd: ì „ì²´ vs ê° subgroup ë¶„í¬ MMDì˜ ìµœëŒ“ê°’.
def sup_mmd_over_V(proba, V, min_support=5, sigma=None):
    """
    ğ’±(list of groups) ìœ„ì—ì„œ sup MMD(ê·¸ë£¹ vs ë³´ì™„) ê³„ì‚°.
    V ì›ì†Œ: bool mask (n,) ë˜ëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ ë°°ì—´.
    """
    p = np.asarray(proba).reshape(-1)
    n = p.size
    masks = _as_mask_list_from_V(V, n)
    if sigma is None:
        sigma = _median_heuristic_sigma(p)
    best = 0.0
    iterator = tqdm(masks, desc="[Performance metric: worst_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns, nsc = int(m.sum()), int((~m).sum())  # ns = ssì— ë“¤ì–´ê°„ ìƒ˜í”Œ ìˆ˜, nsc = ss complementì— ë“¤ì–´ê°„ ìƒ˜í”Œ ìˆ˜
        if ns < min_support or nsc < min_support:
            continue
        v = _mmd2_unbiased(p[m], p[~m], sigma) ** 0.5
        if v > best:
            best = v
    return float(best)



## [ ] 2) wasserstein_statistic(P,Q): ë‘ ë¶„í¬ì˜ WD.
## [ ] 4) sup_wd: ì „ì²´ vs ê° subgroup ë¶„í¬ WDì˜ ìµœëŒ“ê°’.


def _cdf_1d(values):
    """ì •ë ¬Â·ì¤‘ë³µ í¬í•¨í•œ ê°’ê³¼ CDF ê°’ ìŒ ë°˜í™˜. """
    v = np.sort(values.reshape(-1))
    # ê³ ìœ  ì§€ì ì—ì„œì˜ CDF (ê³„ë‹¨í•¨ìˆ˜ ì¢Œì—°ì†)
    uniq, counts = np.unique(v, return_counts=True)
    cdf = np.cumsum(counts) / float(v.size)
    return uniq, cdf

def _w1_1d(x, y):
    """
    Wasserstein-1 (1D) for empirical distributions.
    ìˆ˜ì¹˜ ì ë¶„: CDF ì°¨ì´ì˜ L1 ì ë¶„.
    W1(P,Q) = âˆ« |F_P(t) - F_Q(t)| dt
    W1(\hat{P}_n, \hat{Q}_m) = \sum |F_P(t_i) - F_Q(t_i)| * (t_{i+1}-t_i)
    (t_i: ì ë¶„ì„ summationìœ¼ë¡œ ê·¼ì‚¬í• ë•Œ êµ¬ê°„ ì˜ë¦¬ëŠ” ìƒ˜í”Œë“¤, ì›ë˜ smoothing cdfì¸ë° empirical cdfë¼ ê³„ë‹¨ì‹)
    """
    x = np.asarray(x).reshape(-1)
    y = np.asarray(y).reshape(-1)
    if x.size == 0 or y.size == 0:
        return np.nan
    ux, Fx = _cdf_1d(x)
    uy, Fy = _cdf_1d(y)
    # ê³µí†µ ê²©ìì—ì„œ CDF ë³´ê°„(ê³„ë‹¨í•¨ìˆ˜)
    grid = np.unique(np.concatenate([ux, uy]))
    # Fx(grid), Fy(grid)
    Fx_on = np.interp(grid, ux, Fx, left=0.0, right=1.0)
    Fy_on = np.interp(grid, uy, Fy, left=0.0, right=1.0)
    # ì ë¶„: sum |Î”F| * Î”x
    dx = np.diff(grid)
    dF = np.abs(Fx_on[:-1] - Fy_on[:-1])
    return float(np.sum(dF * dx))


## [ ] 2) wasserstein_statistic(P,Q): ë‘ ë¶„í¬ì˜ WD.
def sup_w1_dfcols(proba, S_df, min_support=5):
    """
    ê° S_df[col]ì„ ê·¸ë£¹ìœ¼ë¡œ ë³´ê³  sup W1(ê·¸ë£¹ vs ë³´ì™„) ê³„ì‚°.
    """
    p = np.asarray(proba).reshape(-1)
    best = 0.0
    for c in S_df.columns:
        m = S_df[c].values.astype(int) == 1
        ns, nsc = int(m.sum()), int((~m).sum())
        if ns < min_support or nsc < min_support:
            continue
        v = _w1_1d(p[m], p[~m])
        if v == v and v > best:
            best = v
    return float(best)

## [ ] 4) sup_wd: ì „ì²´ vs ê° subgroup ë¶„í¬ WDì˜ ìµœëŒ“ê°’.
def sup_w1_over_V(proba, V, min_support=5):
    """
    ğ’±(list of groups) ìœ„ì—ì„œ sup W1(ê·¸ë£¹ vs ë³´ì™„) ê³„ì‚°.
    """
    p = np.asarray(proba).reshape(-1)
    n = p.size
    masks = _as_mask_list_from_V(V, n)
    best = 0.0
    iterator = tqdm(masks, desc="[Performance metric: worst_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns, nsc = int(m.sum()), int((~m).sum()) # ns = ssì— ë“¤ì–´ê°„ ìƒ˜í”Œ ìˆ˜, nsc = ss complementì— ë“¤ì–´ê°„ ìƒ˜í”Œ ìˆ˜
        if ns < min_support or nsc < min_support:
            continue
        v = _w1_1d(p[m], p[~m])
        if v == v and v > best:
            best = v
    return float(best)



## [v] 5,6) worst/mean subgroup_spd: SPD.

def worst_group_spd(proba, S_or_list):
    """max_g |Pr(h=1|g) - Pr(h=1)|; hëŠ” proba>=0.5 ëŒ€ì²´ (ì¸¡ì • ì‹œì—ë§Œ)"""
    arr = (proba>=0.5).astype(int)
    overall = _rate(arr)
    worst = 0.0
    if isinstance(S_or_list, list):
        keys = list(S_or_list[0].keys())
        m_all = {k: np.array([int(s[k]) for s in S_or_list]) for k in keys}
        for k,m in m_all.items():
            a = arr[m==1]; 
            if len(a)>5: worst = max(worst, abs(_rate(a)-overall))
        return float(worst)
    else:
        for c in S_or_list.columns:
            m = S_or_list[c].values.astype(int)
            a = arr[m==1]
            if len(a)>5: worst = max(worst, abs(_rate(a)-overall))
        return float(worst)


def mean_group_spd(proba, S_or_list, thr=0.5, min_support=5):
    """
    í‰ê·  SPD gap: mean_g |Pr(h=1|g) - Pr(h=1)|  (ì¸¡ì •ì‹œì—ë§Œ h = 1(proba>=thr))
    - tabular: S_or_list = DataFrame (0/1 ì»¬ëŸ¼)
    - image:   S_or_list = list[dict] (ê° dictì˜ í‚¤ê°€ ë¯¼ê° ì†ì„±)
    """
    arr = (proba.reshape(-1) >= thr).astype(int)
    overall = float(arr.mean()) if len(arr) > 0 else np.nan
    diffs = []
    if isinstance(S_or_list, list):  # image ìŠ¤íƒ€ì¼
        keys = list(S_or_list[0].keys())
        m_all = {k: np.array([int(s[k]) for s in S_or_list]) for k in keys}
        for k, m in m_all.items():
            a = arr[m == 1]
            if a.size >= min_support:
                diffs.append(abs(float(a.mean()) - overall))
    else:  # DataFrame
        for c in S_or_list.columns:
            m = S_or_list[c].values.astype(int)
            a = arr[m == 1]
            if a.size >= min_support:
                diffs.append(abs(float(a.mean()) - overall))
    return float(np.mean(diffs)) if len(diffs) > 0 else 0.0



## [ ] 7,8) worst/mean weighted_subgroup_spd: ê·¸ë£¹ ë¹ˆë„ë¡œ ê°€ì¤‘ í‰ê· ëœ SPD.


def worst_weighted_group_spd(proba, S_or_list, thr=0.5, min_support=5):
    """
    ê°€ì¤‘ ìµœì•… SPD: max_g  w_g * |Pr(h=1|g) - Pr(h=1)|
    w_g = |g| / n   (ê·¸ë£¹ ë¹ˆë„)
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    if n == 0:
        return np.nan
    overall = float(arr.mean())
    best = 0.0
    if isinstance(S_or_list, list):
        keys = list(S_or_list[0].keys())
        m_all = {k: np.array([int(s[k]) for s in S_or_list]) for k in keys}
        for k, m in m_all.items():
            m = (m == 1)
            ns = int(m.sum())
            if ns < min_support:
                continue
            w = ns / float(n)
            gap = abs(float(arr[m].mean()) - overall)
            best = max(best, w * gap)
    else:
        for c in S_or_list.columns:
            m = (S_or_list[c].values.astype(int) == 1)
            ns = int(m.sum())
            if ns < min_support:
                continue
            w = ns / float(n)
            gap = abs(float(arr[m].mean()) - overall)
            best = max(best, w * gap)
    return float(best)


def mean_weighted_group_spd(proba, S_or_list, thr=0.5, min_support=5):
    """
    ê°€ì¤‘ í‰ê·  SPD: (âˆ‘_g w_g * |Pr(h=1|g) - Pr(h=1)|) / (âˆ‘_g w_g)
    w_g = |g| / n   (ê·¸ë£¹ ë¹ˆë„)
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    if n == 0:
        return np.nan
    overall = float(arr.mean())
    num = 0.0
    den = 0.0
    if isinstance(S_or_list, list):  # image ìŠ¤íƒ€ì¼
        keys = list(S_or_list[0].keys())
        m_all = {k: np.array([int(s[k]) for s in S_or_list]) for k in keys}
        for k, m in m_all.items():
            m = (m == 1)
            ns = int(m.sum())
            if ns < min_support:
                continue
            w = ns / float(n)
            num += w * abs(float(arr[m].mean()) - overall)
            den += w
    else:  # DataFrame
        for c in S_or_list.columns:
            m = (S_or_list[c].values.astype(int) == 1)
            ns = int(m.sum())
            if ns < min_support:
                continue
            w = ns / float(n)
            num += w * abs(float(arr[m].mean()) - overall)
            den += w
    if den == 0.0:
        return 0.0
    return float(num / den)



## [v] 9,10) worst/mean subgroup_subset_spd / dp_sup_V,dp_mean_V : SPD.

def worst_spd_over_V(proba, V, thr=0.5, min_support=5):
    """
    ğ’± ìœ„ì—ì„œ DP sup: max_G |Pr(h=1|G) - Pr(h=1)|.
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    overall = float(arr.mean()) if n > 0 else np.nan
    masks = _as_mask_list_from_V(V, n)
    best = 0.0
    iterator = tqdm(masks, desc="[Performance metric: worst_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns = int(m.sum())
        if ns < min_support:
            continue
        gap = abs(float(arr[m].mean()) - overall)
        if gap > best:
            best = gap
    return float(best)

def mean_spd_over_V(proba, V, thr=0.5, min_support=5):
    """
    ğ’± ìœ„ì—ì„œ DP í‰ê·  ê²©ì°¨.
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    overall = float(arr.mean()) if n > 0 else np.nan
    masks = _as_mask_list_from_V(V, n)
    diffs = []
    iterator = tqdm(masks, desc="[Performance metric: mean_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns = int(m.sum())
        if ns < min_support:
            continue
        diffs.append(abs(float(arr[m].mean()) - overall))
    return float(np.mean(diffs)) if len(diffs) > 0 else 0.0





## [v] 11,12) worst/mean weighted_subgroup_subset_spd: ê·¸ë£¹ ë¹ˆë„ë¡œ ê°€ì¤‘ í‰ê· ëœ SPD

def worst_weighted_spd_over_V(proba, V, thr=0.5, min_support=5):
    """
    ê°€ì¤‘ ìµœì•… SPD over ğ’±: max_G  w_G * |Pr(h=1|G) - Pr(h=1)|
    w_G = |G| / n
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    if n == 0:
        return np.nan
    overall = float(arr.mean())
    masks = _as_mask_list_from_V(V, n)
    best = 0.0
    iterator = tqdm(masks, desc="[Performance metric: worst_weighted_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns = int(m.sum())
        if ns < min_support:
            continue
        w = ns / float(n)
        gap = abs(float(arr[m].mean()) - overall)
        best = max(best, w * gap)
    return float(best)


def mean_weighted_spd_over_V(proba, V, thr=0.5, min_support=5):
    """
    ê°€ì¤‘ í‰ê·  SPD over ğ’±: (âˆ‘_G w_G * |Pr(h=1|G) - Pr(h=1)|) / (âˆ‘_G w_G)
    w_G = |G| / n
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    if n == 0:
        return np.nan
    overall = float(arr.mean())
    masks = _as_mask_list_from_V(V, n)
    num = 0.0
    den = 0.0
    iterator = tqdm(masks, desc="[Performance metric: mean_weighted_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns = int(m.sum())
        if ns < min_support:
            continue
        w = ns / float(n)
        num += w * abs(float(arr[m].mean()) - overall)
        den += w
    if den == 0.0:
        return 0.0
    return float(num / den)





#########################################################
## ê¸°ì¡´ì˜ metrics
#########################################################



def worst_group_fpr_gap(pred, y, S_df):
    """max_g |FPR_g - FPR_all|, FPR = P(h=1|y=0)"""
    pred = pred.reshape(-1); y=y.reshape(-1)
    mask_all = (y==0); 
    if mask_all.sum()<5: return np.nan
    fpr_all = (pred[mask_all]==1).mean()
    worst=0.0
    for c in S_df.columns:
        m = (S_df[c].values==1)&(y==0)
        if m.sum()<5: continue
        fpr = (pred[m]==1).mean()
        worst=max(worst, abs(fpr-fpr_all))
    return float(worst)

def multicalib_worst_gap(proba, y, S_df, bins=10):
    """max_g max_b |E[y|score in bin & g] - avg(score in bin & g)|"""
    proba=proba.reshape(-1); y=y.reshape(-1)
    edges = np.linspace(0,1,bins+1)
    worst=0.0
    for c in S_df.columns:
        m = S_df[c].values==1
        if m.sum()<10: continue
        for i in range(bins):
            lo,hi = edges[i], edges[i+1]
            idx = m & (proba>=lo) & (proba<hi)
            if idx.sum()<10: continue
            gap = abs(y[idx].mean() - proba[idx].mean())
            worst=max(worst, float(gap))
    return worst if worst>0 else 0.0


def mean_group_fpr_gap(pred, y, S_df, min_support=5):
    """
    í‰ê·  FPR gap: mean_g |FPR_g - FPR_all|,  FPR = P(h=1 | y=0)
    - pred: {0,1} ì˜ˆì¸¡ (ì´ë¯¸ threshold ì ìš©ëœ ê°’ ì‚¬ìš© ê¶Œì¥)
    - y: {0,1} ë¼ë²¨
    - S_df: ê° ì»¬ëŸ¼ì´ 0/1ì¸ ê·¸ë£¹ ë§ˆìŠ¤í¬
    """
    pred = pred.reshape(-1)
    y = y.reshape(-1)
    mask_all = (y == 0)
    if mask_all.sum() < min_support:
        return np.nan
    fpr_all = float((pred[mask_all] == 1).mean())

    gaps = []
    for c in S_df.columns:
        m = (S_df[c].values == 1) & (y == 0)
        if m.sum() < min_support:
            continue
        fpr = float((pred[m] == 1).mean())
        gaps.append(abs(fpr - fpr_all))
    return float(np.mean(gaps)) if len(gaps) > 0 else 0.0


def multicalib_mean_gap(proba, y, S_df, bins=10, min_support=10):
    """
    í‰ê·  multicalibration gap:
      mean_{g,b} | E[y | p in bin & g] - E[p | p in bin & g] |
    - proba âˆˆ [0,1], y âˆˆ {0,1}
    - binë§ˆë‹¤/ê·¸ë£¹ë§ˆë‹¤ í‘œë³¸ì´ min_support ì´ìƒì¼ ë•Œë§Œ í¬í•¨
    """
    proba = proba.reshape(-1)
    y = y.reshape(-1)
    edges = np.linspace(0, 1, bins + 1)
    gaps = []
    for c in S_df.columns:
        m_g = (S_df[c].values == 1)
        if m_g.sum() < min_support:
            continue
        for i in range(bins):
            lo, hi = edges[i], edges[i + 1]
            idx = m_g & (proba >= lo) & (proba < hi if i < bins - 1 else proba <= hi)
            if idx.sum() < min_support:
                continue
            gaps.append(abs(float(y[idx].mean()) - float(proba[idx].mean())))
    return float(np.mean(gaps)) if len(gaps) > 0 else 0.0



# =====================================================================
# ============== ì—¬ê¸°ë¶€í„° Marginal(ì†ì„±ë³„) ë©”íŠ¸ë¦­ ì¶”ê°€ =================
# =====================================================================

def _families_from_S(S_or_list):
    """
    Së¥¼ ì†ì„± familyë¡œ ë¬¶ëŠ” í—¬í¼.
    - DataFrame: 'attr_value' í˜•íƒœì˜ ì»¬ëŸ¼ì€ 'attr'ë¥¼ familyë¡œ ë¬¶ìŒ.
                 '_'ê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ì»¬ëŸ¼ëª…ì´ ê³§ family(ì´ì§„).
    - list[dict] (image): ê° keyê°€ í•˜ë‚˜ì˜ ì´ì§„ ì†ì„± â†’ family[key] = [key]
    return: dict[str, list[str]]  (family -> list of columns/keys)
    """
    fam = {}
    if isinstance(S_or_list, list):
        keys = list(S_or_list[0].keys())
        for k in keys:
            fam[k] = [k]
    else:
        for c in S_or_list.columns:
            parts = str(c).split("_", 1)
            family = parts[0] if len(parts) > 1 else str(c)
            fam.setdefault(family, []).append(c)
    return fam

def _mask_from_key(S_or_list, key):
    if isinstance(S_or_list, list):
        return np.array([int(s[key]) for s in S_or_list])
    else:
        return S_or_list[key].values.astype(int)

def _spd_for_family(arr01, S_or_list, keys, min_support=5):
    """
    family ì•ˆì—ì„œì˜ SPD ì •ì˜:
      - keysê°€ 1ê°œ(ì´ì§„ ì†ì„±): |Pr(h=1|S=1) - Pr(h=1|S=0)|
      - keysê°€ ì—¬ëŸ¬ ê°œ(ì›-í•« ì¹´í…Œê³ ë¦¬): max_{i,j} |Pr(h=1|k_i) - Pr(h=1|k_j)|
    """
    if len(keys) == 1:
        m = _mask_from_key(S_or_list, keys[0]) == 1
        n1, n0 = m.sum(), (~m).sum()
        if n1 >= min_support and n0 >= min_support:
            return abs(float(arr01[m].mean()) - float(arr01[~m].mean()))
        return np.nan
    # multi-cat
    rates = []
    for k in keys:
        m = _mask_from_key(S_or_list, k) == 1
        if m.sum() >= min_support:
            rates.append(float(arr01[m].mean()))
    if len(rates) < 2:
        return np.nan
    return float(np.max(rates) - np.min(rates))

def _fpr_for_family(pred01, y, S_or_list, keys, min_support=5):
    """
    family ì•ˆì—ì„œì˜ FPR ì •ì˜(ìœ„ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ):
      - binary: |FPR(S=1) - FPR(S=0)|
      - multi-cat: max_{i,j} |FPR(k_i) - FPR(k_j)|
    """
    y = y.reshape(-1)
    pred01 = pred01.reshape(-1)
    mask_all = (y == 0)
    if mask_all.sum() < min_support:
        return np.nan

    def fpr_of(m):
        idx = mask_all & (m == 1)
        if idx.sum() < min_support:
            return None
        return float((pred01[idx] == 1).mean())

    if len(keys) == 1:
        m = _mask_from_key(S_or_list, keys[0])
        f1 = fpr_of(m)
        f0 = fpr_of(1 - m)
        if f1 is None or f0 is None:
            return np.nan
        return abs(f1 - f0)

    vals = []
    for k in keys:
        m = _mask_from_key(S_or_list, k)
        f = fpr_of(m)
        if f is not None:
            vals.append(f)
    if len(vals) < 2:
        return np.nan
    return float(np.max(vals) - np.min(vals))

def _mc_for_family(proba, y, S_or_list, keys, bins=10, min_support=10):
    """
    family ì•ˆì—ì„œì˜ multicalibration worst gap:
      - binary: max_b gap(S=1,b), gap(S=0,b) ì¤‘ ìµœëŒ€
      - multi-cat: max_{k in keys} max_b gap(k,b)
    """
    proba = proba.reshape(-1)
    y = y.reshape(-1)
    edges = np.linspace(0, 1, bins + 1)

    def max_gap_of_mask(m):
        worst = 0.0
        for i in range(bins):
            lo, hi = edges[i], edges[i + 1]
            idx = (m == 1) & (proba >= lo) & (proba < (hi if i < bins - 1 else proba.max() + 1e-12))
            if idx.sum() < min_support:
                continue
            worst = max(worst, abs(float(y[idx].mean()) - float(proba[idx].mean())))
        return worst if worst > 0 else None

    gaps = []
    if len(keys) == 1:
        m = _mask_from_key(S_or_list, keys[0])
        g1 = max_gap_of_mask(m)
        g0 = max_gap_of_mask(1 - m)
        for g in (g1, g0):
            if g is not None:
                gaps.append(g)
    else:
        for k in keys:
            m = _mask_from_key(S_or_list, k)
            g = max_gap_of_mask(m)
            if g is not None:
                gaps.append(g)

    if len(gaps) == 0:
        return np.nan
    return float(np.max(gaps))



# -------- Marginal SPD --------
def marginal_spd_worst(proba, S_or_list, thr=0.5, min_support=5, families=None):
    arr01 = (proba.reshape(-1) >= thr).astype(int)
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _spd_for_family(arr01, S_or_list, keys, min_support=min_support)
        if v == v:  # not NaN
            vals.append(v)
    return float(np.max(vals)) if len(vals) else 0.0

def marginal_spd_mean(proba, S_or_list, thr=0.5, min_support=5, families=None):
    arr01 = (proba.reshape(-1) >= thr).astype(int)
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _spd_for_family(arr01, S_or_list, keys, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.mean(vals)) if len(vals) else 0.0

# -------- Marginal FPR --------
def marginal_fpr_worst(pred, y, S_or_list, min_support=5, families=None):
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _fpr_for_family(pred, y, S_or_list, keys, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.max(vals)) if len(vals) else 0.0

def marginal_fpr_mean(pred, y, S_or_list, min_support=5, families=None):
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _fpr_for_family(pred, y, S_or_list, keys, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.mean(vals)) if len(vals) else 0.0

# -------- Marginal Multicalibration --------
def marginal_mc_worst(proba, y, S_or_list, bins=10, min_support=10, families=None):
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _mc_for_family(proba, y, S_or_list, keys, bins=bins, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.max(vals)) if len(vals) else 0.0

def marginal_mc_mean(proba, y, S_or_list, bins=10, min_support=10, families=None):
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _mc_for_family(proba, y, S_or_list, keys, bins=bins, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.mean(vals)) if len(vals) else 0.0