# fairbench/metrics/subgroup_measures.py
import numpy as np
from tqdm import tqdm

## ê±´ì›…ì´ê°€ ë„˜ê²¨ì¤€ê±°: SPD/FPR/MC (worst, mean, marginal)
#########################################################
## ìš°ë¦¬ê°€ ì‹¤í—˜í•  metrics (subgroup-based)
## 0-1 ) sup_mmd: subgroup vs ì „ì²´ ë¶„í¬ MMD ì°¨ì´ ìµœëŒ“ê°’
## 0-2 ) sup_w1: subgroup vs ì „ì²´ ë¶„í¬     Wasserstein-1 (1D) for empirical distributions. ì°¨ì´ ìµœëŒ“ê°’
## [vv] 1) sup_mmd_dfcols: subgroup vs complement MMD.                                                                  - singleton subgroup
## [vv] 2) sup_w1_dfcols: subgroup vs complement WD.                                                                     - singleton subgroup
## [vv] 3) sup_mmd_over_V: subgroup subset vs complement MMD ì°¨ì´ì˜ ìµœëŒ“ê°’.                                           - ğ’± (subgroup subset)
## [vv] 4) sup_w1_over_V: subgroup subset vs complement WD ì°¨ì´ì˜ ìµœëŒ“ê°’.                                             - ğ’± (subgroup subset)

## [v] 5,6) worst/mean worst_group_spd, mean_group_spd: SPD.                                                - singleton subgroup
## [vv] 7,8) worst/mean worst_weighted_group_spd, mean_weighted_group_spd : ê·¸ë£¹ ë¹ˆë„ë¡œ ê°€ì¤‘ í‰ê· ëœ SPD.          - singleton subgroup
## [v] 9,10) worst/mean worst_spd_over_V,mean_spd_over_V : SPD.                                                - ğ’± (subgroup subset)
## [vv] 11,12) worst/mean worst_weighted_spd_over_V, mean_weighted_spd_over_V: ê·¸ë£¹ ë¹ˆë„ë¡œ ê°€ì¤‘ í‰ê· ëœ SPD.        - ğ’± (subgroup subset)

#########################################################
## ê¸°ì¡´ subgroup ë©”íŠ¸ë¦­ (worst/mean)
## 1) worst/mean subgroup/marginal FPR gap
## 2) worst/mean subgroup/marginal multicalibration gap
#########################################################

import numpy as np, random, torch
random.seed(0)
np.random.seed(0)
torch.manual_seed(0)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(0)

# parameter
# min_support: ê·¸ë£¹ì— ì´ê±°ë³´ë‹¤ ìƒ˜í”Œ ìˆ˜ ì ìœ¼ë©´ metric ê³„ì‚° ì•ˆí• ê²Œ
# sigma: MMDì— ìˆëŠ” RBF kernel bandwidth (Noneì´ë©´ ìƒ˜í”Œê°„ ê±°ë¦¬ì˜ ì¤‘ì•™ê°’ì„ ì“°ê² ë‹¤)

import numpy as np


def _rate(pred):
    return float(np.mean(pred)) if len(pred) > 0 else np.nan

# ---------- êµì°¨(2^q) ê·¸ë£¹ ìœ í‹¸ ----------
def _to_bin_matrix(S_or_list):
    """S_or_list -> (n,q) binary ndarray (0/1)"""
    if isinstance(S_or_list, list):
        keys = list(S_or_list[0].keys())
        A = np.array([[int(s[k]) for k in keys] for s in S_or_list], dtype=int)
    else:
        A = np.asarray(S_or_list.values)
        if A.ndim == 1:
            A = A[:, None]
        A = np.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)
        A = np.rint(A).astype(int)
    uniq_vals = np.unique(A)
    if not set(uniq_vals.tolist()).issubset({0, 1}):
        raise ValueError(f"S must be binary (0/1). Got values {uniq_vals}.")
    return A

def _ensure_same_n(n_vec, S_bin):
    if S_bin.shape[0] != n_vec:
        raise ValueError(
            f"Length mismatch: vector has n={n_vec} but S has n={S_bin.shape[0]}. "
            "Make sure you pass the *test* S (not stacked) matching proba/pred/y."
        )

def _full_group_ids(S_bin):
    """(n,q) 0/1 -> gid (n,), total groups G=2^q"""
    q = S_bin.shape[1]
    coeff = (1 << np.arange(q, dtype=np.int64))  # 1,2,4,8,...
    gid = S_bin.astype(np.int64) @ coeff
    G = 1 << q
    return gid, G



def _as_mask_list_from_V(V, n):
    """
    subgroup subset ì´ inputìœ¼ë¡œ ë“¤ì–´ì˜¤ë©´ ê° ìƒ˜í”Œì´ í•´ë‹¹ ssì— ë“¤ì–´ê°€ëŠ”ì§€ ì—¬ë¶€ ë°˜í™˜
    subgroup subset 10ê°œ ë“¤ì–´ì˜¤ë©´ ê° ìƒ˜í”Œì˜ 10ê°œ bool mask ì¶œë ¥
    V: list of group specs. ê° ì›ì†ŒëŠ”
       - bool mask (shape (n,))
       - ì •ìˆ˜ ì¸ë±ìŠ¤ ë°°ì—´
    return: list[np.ndarray(bool, shape (n,))]
    """
    masks = []
    for g in V:
        if isinstance(g, np.ndarray) and g.dtype == bool:
            m = g
        else:
            idx = np.asarray(g, dtype=int).reshape(-1)
            m = np.zeros(n, dtype=bool)
            m[idx] = True
        masks.append(m)
    return masks


def _rate(pred):
    return float(np.mean(pred)) if len(pred)>0 else np.nan


## [ ] 1) mmd_statistic(P,Q): ë‘ ë¶„í¬ì˜ MMD.
## [ ] 3) sup_mmd: ì „ì²´ vs ê° subgroup ë¶„í¬ MMDì˜ ìµœëŒ“ê°’.


def _pairwise_rbf_sum(x, sigma):
    """
    âˆ‘_{i<j} k(x_i,x_j) with RBF. (self-term ì œì™¸)
    E[k(x,x')] ê³„ì‚°í•˜ëŠ”ì• 
    """
    x = x.reshape(-1, 1)
    # (x_i - x_j)^2 = (x^2) + (x^2)^T - 2 x x^T
    x2 = (x ** 2).sum(axis=1, keepdims=True)
    d2 = x2 + x2.T - 2.0 * (x @ x.T)
    K = np.exp(-d2 / (2.0 * sigma * sigma))
    # ëŒ€ê°ì„ 0ìœ¼ë¡œ
    np.fill_diagonal(K, 0.0)
    # ìƒì‚¼ê° í•©
    return float(K.sum() / 2.0)

def _mmd2_unbiased(x, y, sigma):
    """
    Unbiased MMD^2 for RBF kernel (Gretton et al.).
    MMD^2 = E[k(x,x')] + E[k(y,y')] - 2E[k(x,y)]
    """
    x = np.asarray(x).reshape(-1)
    y = np.asarray(y).reshape(-1)
    nx, ny = x.size, y.size
    if nx < 2 or ny < 2:
        return np.nan
    k_xx = _pairwise_rbf_sum(x, sigma)
    k_yy = _pairwise_rbf_sum(y, sigma)
    # êµí•­: ëª¨ë“  (i in x, j in y)
    x_ = x.reshape(-1, 1)
    y_ = y.reshape(1, -1)
    d2 = (x_ ** 2) + (y_ ** 2) - 2.0 * (x_ @ y_)
    k_xy = np.exp(-d2 / (2.0 * sigma * sigma)).sum()
    mmd2 = (2.0 / (nx * (nx - 1))) * k_xx \
         + (2.0 / (ny * (ny - 1))) * k_yy \
         - (2.0 / (nx * ny)) * k_xy
    return float(max(mmd2, 0.0))

def _median_heuristic_sigma(z):
    """
    median heuristic for 1D scores.
    """
    z = np.asarray(z).reshape(-1)
    if z.size < 2:
        return 1.0
    zz = np.sort(z)
    d = np.abs(zz[1:] - zz[:-1])
    med = np.median(d)
    return float(max(med, 1e-3))

## [ ] 1) mmd_statistic(P,Q): ë‘ ë¶„í¬ì˜ MMD.
def sup_mmd_dfcols(proba, S_df, sigma=None, min_support=5):
    """
    ê° S_df[col]ì„ ê·¸ë£¹ìœ¼ë¡œ ë³´ê³  sup MMD(ê·¸ë£¹ vs ë³´ì™„) ê³„ì‚°.
    """
    p = np.asarray(proba).reshape(-1)
    n = p.size
    if sigma is None:
        sigma = _median_heuristic_sigma(p)
    best = 0.0
    for c in S_df.columns:
        m = S_df[c].values.astype(int) == 1
        ns, nsc = int(m.sum()), int((~m).sum())
        if ns < min_support or nsc < min_support:
            continue
        v = _mmd2_unbiased(p[m], p[~m], sigma) ** 0.5  # MMD (not squared)
        if v > best:
            best = v
    return float(best)

## [ ] 3) sup_mmd: ì „ì²´ vs ê° subgroup ë¶„í¬ MMDì˜ ìµœëŒ“ê°’.
def sup_mmd_over_V(proba, V, min_support=5, sigma=None):
    """
    ğ’±(list of groups) ìœ„ì—ì„œ sup MMD(ê·¸ë£¹ vs ë³´ì™„) ê³„ì‚°.
    V ì›ì†Œ: bool mask (n,) ë˜ëŠ” ì •ìˆ˜ ì¸ë±ìŠ¤ ë°°ì—´.
    """
    p = np.asarray(proba).reshape(-1)
    n = p.size
    masks = _as_mask_list_from_V(V, n)
    if sigma is None:
        sigma = _median_heuristic_sigma(p)
    best = 0.0
    iterator = tqdm(masks, desc="[Performance metric: worst_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns, nsc = int(m.sum()), int((~m).sum())  # ns = ssì— ë“¤ì–´ê°„ ìƒ˜í”Œ ìˆ˜, nsc = ss complementì— ë“¤ì–´ê°„ ìƒ˜í”Œ ìˆ˜
        if ns < min_support or nsc < min_support:
            continue
        v = _mmd2_unbiased(p[m], p[~m], sigma) ** 0.5
        if v > best:
            best = v
    return float(best)



## [ ] 2) wasserstein_statistic(P,Q): ë‘ ë¶„í¬ì˜ WD.
## [ ] 4) sup_wd: ì „ì²´ vs ê° subgroup ë¶„í¬ WDì˜ ìµœëŒ“ê°’.


def _cdf_1d(values):
    """ì •ë ¬Â·ì¤‘ë³µ í¬í•¨í•œ ê°’ê³¼ CDF ê°’ ìŒ ë°˜í™˜. """
    v = np.sort(values.reshape(-1))
    # ê³ ìœ  ì§€ì ì—ì„œì˜ CDF (ê³„ë‹¨í•¨ìˆ˜ ì¢Œì—°ì†)
    uniq, counts = np.unique(v, return_counts=True)
    cdf = np.cumsum(counts) / float(v.size)
    return uniq, cdf

def _w1_1d(x, y):
    """
    Wasserstein-1 (1D) for empirical distributions.
    ìˆ˜ì¹˜ ì ë¶„: CDF ì°¨ì´ì˜ L1 ì ë¶„.
    W1(P,Q) = âˆ« |F_P(t) - F_Q(t)| dt
    W1(\hat{P}_n, \hat{Q}_m) = \sum |F_P(t_i) - F_Q(t_i)| * (t_{i+1}-t_i)
    (t_i: ì ë¶„ì„ summationìœ¼ë¡œ ê·¼ì‚¬í• ë•Œ êµ¬ê°„ ì˜ë¦¬ëŠ” ìƒ˜í”Œë“¤, ì›ë˜ smoothing cdfì¸ë° empirical cdfë¼ ê³„ë‹¨ì‹)
    """
    x = np.asarray(x).reshape(-1)
    y = np.asarray(y).reshape(-1)
    if x.size == 0 or y.size == 0:
        return np.nan
    ux, Fx = _cdf_1d(x)
    uy, Fy = _cdf_1d(y)
    # ê³µí†µ ê²©ìì—ì„œ CDF ë³´ê°„(ê³„ë‹¨í•¨ìˆ˜)
    grid = np.unique(np.concatenate([ux, uy]))
    # Fx(grid), Fy(grid)
    Fx_on = np.interp(grid, ux, Fx, left=0.0, right=1.0)
    Fy_on = np.interp(grid, uy, Fy, left=0.0, right=1.0)
    # ì ë¶„: sum |Î”F| * Î”x
    dx = np.diff(grid)
    dF = np.abs(Fx_on[:-1] - Fy_on[:-1])
    return float(np.sum(dF * dx))


## [ ] 2) wasserstein_statistic(P,Q): ë‘ ë¶„í¬ì˜ WD.
def sup_w1_dfcols(proba, S_df, min_support=5):
    """
    ê° S_df[col]ì„ ê·¸ë£¹ìœ¼ë¡œ ë³´ê³  sup W1(ê·¸ë£¹ vs ë³´ì™„) ê³„ì‚°.
    """
    p = np.asarray(proba).reshape(-1)
    best = 0.0
    for c in S_df.columns:
        m = S_df[c].values.astype(int) == 1
        ns, nsc = int(m.sum()), int((~m).sum())
        if ns < min_support or nsc < min_support:
            continue
        v = _w1_1d(p[m], p[~m])
        if v == v and v > best:
            best = v
    return float(best)

## [ ] 4) sup_wd: ì „ì²´ vs ê° subgroup ë¶„í¬ WDì˜ ìµœëŒ“ê°’.
def sup_w1_over_V(proba, V, min_support=5):
    """
    ğ’±(list of groups) ìœ„ì—ì„œ sup W1(ê·¸ë£¹ vs ë³´ì™„) ê³„ì‚°.
    """
    p = np.asarray(proba).reshape(-1)
    n = p.size
    masks = _as_mask_list_from_V(V, n)
    best = 0.0
    iterator = tqdm(masks, desc="[Performance metric: worst_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns, nsc = int(m.sum()), int((~m).sum()) # ns = ssì— ë“¤ì–´ê°„ ìƒ˜í”Œ ìˆ˜, nsc = ss complementì— ë“¤ì–´ê°„ ìƒ˜í”Œ ìˆ˜
        if ns < min_support or nsc < min_support:
            continue
        v = _w1_1d(p[m], p[~m])
        if v == v and v > best:
            best = v
    return float(best)



## [v] 5,6) worst/mean subgroup_spd: SPD.

# def worst_group_spd(proba, S_or_list):
#     """max_g |Pr(h=1|g) - Pr(h=1)|; hëŠ” proba>=0.5 ëŒ€ì²´ (ì¸¡ì • ì‹œì—ë§Œ)"""
#     arr = (proba>=0.5).astype(int)
#     overall = _rate(arr)
#     worst = 0.0
#     if isinstance(S_or_list, list):
#         keys = list(S_or_list[0].keys())
#         m_all = {k: np.array([int(s[k]) for s in S_or_list]) for k in keys}
#         for k,m in m_all.items():
#             a = arr[m==1]; 
#             if len(a)>5: worst = max(worst, abs(_rate(a)-overall))
#         return float(worst)
#     else:
#         for c in S_or_list.columns:
#             m = S_or_list[c].values.astype(int)
#             a = arr[m==1]
#             if len(a)>5: worst = max(worst, abs(_rate(a)-overall))
#         return float(worst)


# def mean_group_spd(proba, S_or_list, thr=0.5, min_support=5):
#     """
#     í‰ê·  SPD gap: mean_g |Pr(h=1|g) - Pr(h=1)|  (ì¸¡ì •ì‹œì—ë§Œ h = 1(proba>=thr))
#     - tabular: S_or_list = DataFrame (0/1 ì»¬ëŸ¼)
#     - image:   S_or_list = list[dict] (ê° dictì˜ í‚¤ê°€ ë¯¼ê° ì†ì„±)
#     """
#     arr = (proba.reshape(-1) >= thr).astype(int)
#     overall = float(arr.mean()) if len(arr) > 0 else np.nan
#     diffs = []
#     if isinstance(S_or_list, list):  # image ìŠ¤íƒ€ì¼
#         keys = list(S_or_list[0].keys())
#         m_all = {k: np.array([int(s[k]) for s in S_or_list]) for k in keys}
#         for k, m in m_all.items():
#             a = arr[m == 1]
#             if a.size >= min_support:
#                 diffs.append(abs(float(a.mean()) - overall))
#     else:  # DataFrame
#         for c in S_or_list.columns:
#             m = S_or_list[c].values.astype(int)
#             a = arr[m == 1]
#             if a.size >= min_support:
#                 diffs.append(abs(float(a.mean()) - overall))
#     return float(np.mean(diffs)) if len(diffs) > 0 else 0.0

def worst_group_spd(proba, S_or_list, thr=0.5, min_support=1):
    p = np.asarray(proba, float).reshape(-1)
    yhat = (np.nan_to_num(p, nan=-np.inf) >= thr).astype(int)
    if yhat.size == 0: return 0.0

    S_bin = _to_bin_matrix(S_or_list)
    if S_bin.shape[0] != yhat.shape[0]:
        raise ValueError(f"n mismatch: proba {yhat.shape[0]} vs S {S_bin.shape[0]}")
    gid, G = _full_group_ids(S_bin)

    overall = float(yhat.mean())
    cnt = np.bincount(gid, minlength=G)
    s1  = np.bincount(gid, weights=yhat, minlength=G)
    with np.errstate(divide="ignore", invalid="ignore"):
        mean_g = np.where(cnt>0, s1/cnt, np.nan)
    diffs = np.abs(mean_g - overall)
    mask  = (cnt >= min_support) & np.isfinite(diffs)
    return float(np.nanmax(diffs[mask])) if np.any(mask) else 0.0

def mean_group_spd(proba, S_or_list, thr=0.5, min_support=1):
    p = np.asarray(proba, float).reshape(-1)
    yhat = (np.nan_to_num(p, nan=-np.inf) >= thr).astype(int)
    if yhat.size == 0: return 0.0

    S_bin = _to_bin_matrix(S_or_list)
    if S_bin.shape[0] != yhat.shape[0]:
        raise ValueError(f"n mismatch: proba {yhat.shape[0]} vs S {S_bin.shape[0]}")
    gid, G = _full_group_ids(S_bin)

    overall = float(yhat.mean())
    cnt = np.bincount(gid, minlength=G)
    s1  = np.bincount(gid, weights=yhat, minlength=G)
    with np.errstate(divide="ignore", invalid="ignore"):
        mean_g = np.where(cnt>0, s1/cnt, np.nan)
    diffs = np.abs(mean_g - overall)
    mask  = (cnt >= min_support) & np.isfinite(diffs)
    return float(np.nanmean(diffs[mask])) if np.any(mask) else 0.0




## [ ] 7,8) worst/mean weighted_subgroup_spd: ê·¸ë£¹ ë¹ˆë„ë¡œ ê°€ì¤‘ í‰ê· ëœ SPD.


def worst_weighted_group_spd(proba, S_or_list, thr=0.5, min_support=5):
    """
    ê°€ì¤‘ ìµœì•… SPD: max_g  w_g * |Pr(h=1|g) - Pr(h=1)|
    w_g = |g| / n   (ê·¸ë£¹ ë¹ˆë„)
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    if n == 0:
        return np.nan
    overall = float(arr.mean())
    best = 0.0
    if isinstance(S_or_list, list):
        keys = list(S_or_list[0].keys())
        m_all = {k: np.array([int(s[k]) for s in S_or_list]) for k in keys}
        for k, m in m_all.items():
            m = (m == 1)
            ns = int(m.sum())
            if ns < min_support:
                continue
            w = ns / float(n)
            gap = abs(float(arr[m].mean()) - overall)
            best = max(best, w * gap)
    else:
        for c in S_or_list.columns:
            m = (S_or_list[c].values.astype(int) == 1)
            ns = int(m.sum())
            if ns < min_support:
                continue
            w = ns / float(n)
            gap = abs(float(arr[m].mean()) - overall)
            best = max(best, w * gap)
    return float(best)


def mean_weighted_group_spd(proba, S_or_list, thr=0.5, min_support=5):
    """
    ê°€ì¤‘ í‰ê·  SPD: (âˆ‘_g w_g * |Pr(h=1|g) - Pr(h=1)|) / (âˆ‘_g w_g)
    w_g = |g| / n   (ê·¸ë£¹ ë¹ˆë„)
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    if n == 0:
        return np.nan
    overall = float(arr.mean())
    num = 0.0
    den = 0.0
    if isinstance(S_or_list, list):  # image ìŠ¤íƒ€ì¼
        keys = list(S_or_list[0].keys())
        m_all = {k: np.array([int(s[k]) for s in S_or_list]) for k in keys}
        for k, m in m_all.items():
            m = (m == 1)
            ns = int(m.sum())
            if ns < min_support:
                continue
            w = ns / float(n)
            num += w * abs(float(arr[m].mean()) - overall)
            den += w
    else:  # DataFrame
        for c in S_or_list.columns:
            m = (S_or_list[c].values.astype(int) == 1)
            ns = int(m.sum())
            if ns < min_support:
                continue
            w = ns / float(n)
            num += w * abs(float(arr[m].mean()) - overall)
            den += w
    if den == 0.0:
        return 0.0
    return float(num / den)



## [v] 9,10) worst/mean subgroup_subset_spd / dp_sup_V,dp_mean_V : SPD.

def worst_spd_over_V(proba, V, thr=0.5, min_support=5):
    """
    ğ’± ìœ„ì—ì„œ DP sup: max_G |Pr(h=1|G) - Pr(h=1)|.
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    overall = float(arr.mean()) if n > 0 else np.nan
    masks = _as_mask_list_from_V(V, n)
    best = 0.0
    iterator = tqdm(masks, desc="[Performance metric: worst_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns = int(m.sum())
        if ns < min_support:
            continue
        gap = abs(float(arr[m].mean()) - overall)
        if gap > best:
            best = gap
    return float(best)

def mean_spd_over_V(proba, V, thr=0.5, min_support=5):
    """
    ğ’± ìœ„ì—ì„œ DP í‰ê·  ê²©ì°¨.
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    overall = float(arr.mean()) if n > 0 else np.nan
    masks = _as_mask_list_from_V(V, n)
    diffs = []
    iterator = tqdm(masks, desc="[Performance metric: mean_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns = int(m.sum())
        if ns < min_support:
            continue
        diffs.append(abs(float(arr[m].mean()) - overall))
    return float(np.mean(diffs)) if len(diffs) > 0 else 0.0





## [v] 11,12) worst/mean weighted_subgroup_subset_spd: ê·¸ë£¹ ë¹ˆë„ë¡œ ê°€ì¤‘ í‰ê· ëœ SPD

def worst_weighted_spd_over_V(proba, V, thr=0.5, min_support=5):
    """
    ê°€ì¤‘ ìµœì•… SPD over ğ’±: max_G  w_G * |Pr(h=1|G) - Pr(h=1)|
    w_G = |G| / n
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    if n == 0:
        return np.nan
    overall = float(arr.mean())
    masks = _as_mask_list_from_V(V, n)
    best = 0.0
    iterator = tqdm(masks, desc="[Performance metric: worst_weighted_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns = int(m.sum())
        if ns < min_support:
            continue
        w = ns / float(n)
        gap = abs(float(arr[m].mean()) - overall)
        best = max(best, w * gap)
    return float(best)


def mean_weighted_spd_over_V(proba, V, thr=0.5, min_support=5):
    """
    ê°€ì¤‘ í‰ê·  SPD over ğ’±: (âˆ‘_G w_G * |Pr(h=1|G) - Pr(h=1)|) / (âˆ‘_G w_G)
    w_G = |G| / n
    """
    arr = (np.asarray(proba).reshape(-1) >= float(thr)).astype(int)
    n = arr.size
    if n == 0:
        return np.nan
    overall = float(arr.mean())
    masks = _as_mask_list_from_V(V, n)
    num = 0.0
    den = 0.0
    iterator = tqdm(masks, desc="[Performance metric: mean_weighted_spd_over_V]") if len(masks) > 50 else masks
    for m in iterator:
        ns = int(m.sum())
        if ns < min_support:
            continue
        w = ns / float(n)
        num += w * abs(float(arr[m].mean()) - overall)
        den += w
    if den == 0.0:
        return 0.0
    return float(num / den)





#########################################################
## ê¸°ì¡´ì˜ metrics
#########################################################


def worst_group_fpr_gap(pred, y, S_or_list, min_support=1):
    pred = np.asarray(pred).reshape(-1).astype(int)
    y    = np.asarray(y).reshape(-1).astype(int)
    if pred.size == 0 or y.size == 0: return 0.0
    if pred.shape[0] != y.shape[0]:
        raise ValueError(f"n mismatch: pred {pred.shape[0]} vs y {y.shape[0]}")

    S_bin = _to_bin_matrix(S_or_list)
    if S_bin.shape[0] != pred.shape[0]:
        raise ValueError(f"n mismatch: pred {pred.shape[0]} vs S {S_bin.shape[0]}")
    gid, G = _full_group_ids(S_bin)

    y0 = (y == 0)
    if y0.sum() < min_support: return 0.0
    fpr_all = float((pred[y0] == 1).mean())

    gid0 = gid[y0]
    cnt0 = np.bincount(gid0, minlength=G)
    s10  = np.bincount(gid0, weights=(pred[y0] == 1).astype(float), minlength=G)
    with np.errstate(divide="ignore", invalid="ignore"):
        fpr_g = np.where(cnt0>0, s10/cnt0, np.nan)
    diffs = np.abs(fpr_g - fpr_all)
    mask  = (cnt0 >= min_support) & np.isfinite(diffs)
    return float(np.nanmax(diffs[mask])) if np.any(mask) else 0.0


def multicalib_worst_gap(proba, y, S_or_list, bins=10, min_support=1):
    proba = np.asarray(proba, float).reshape(-1)
    y     = np.asarray(y).reshape(-1).astype(float)
    if proba.size == 0 or y.size == 0: return 0.0
    if proba.shape[0] != y.shape[0]:
        raise ValueError(f"n mismatch: proba {proba.shape[0]} vs y {y.shape[0]}")

    S_bin = _to_bin_matrix(S_or_list)
    if S_bin.shape[0] != proba.shape[0]:
        raise ValueError(f"n mismatch: proba {proba.shape[0]} vs S {S_bin.shape[0]}")
    gid, G = _full_group_ids(S_bin)

    # bin ì¸ë±ìŠ¤ (0..bins-1), ë§ˆì§€ë§‰ binì€ 1.0 í¬í•¨
    b = np.clip((proba * bins).astype(int), 0, bins - 1)
    key = gid * bins + b
    size = G * bins

    cnt = np.bincount(key, minlength=size)
    sy  = np.bincount(key, weights=y,      minlength=size)
    sp  = np.bincount(key, weights=proba,  minlength=size)

    with np.errstate(divide="ignore", invalid="ignore"):
        ey = np.where(cnt>0, sy/cnt, np.nan)
        ep = np.where(cnt>0, sp/cnt, np.nan)
        gap = np.abs(ey - ep)

    valid = (cnt >= min_support) & np.isfinite(gap)
    return float(np.nanmax(gap[valid])) if np.any(valid) else 0.0

def mean_group_fpr_gap(pred, y, S_or_list, min_support=1):
    pred = np.asarray(pred).reshape(-1).astype(int)
    y    = np.asarray(y).reshape(-1).astype(int)
    if pred.size == 0 or y.size == 0: return 0.0
    if pred.shape[0] != y.shape[0]:
        raise ValueError(f"n mismatch: pred {pred.shape[0]} vs y {y.shape[0]}")

    S_bin = _to_bin_matrix(S_or_list)
    if S_bin.shape[0] != pred.shape[0]:
        raise ValueError(f"n mismatch: pred {pred.shape[0]} vs S {S_bin.shape[0]}")
    gid, G = _full_group_ids(S_bin)

    y0 = (y == 0)
    if y0.sum() < min_support: return 0.0
    fpr_all = float((pred[y0] == 1).mean())

    gid0 = gid[y0]
    cnt0 = np.bincount(gid0, minlength=G)
    s10  = np.bincount(gid0, weights=(pred[y0] == 1).astype(float), minlength=G)
    with np.errstate(divide="ignore", invalid="ignore"):
        fpr_g = np.where(cnt0>0, s10/cnt0, np.nan)
    diffs = np.abs(fpr_g - fpr_all)
    mask  = (cnt0 >= min_support) & np.isfinite(diffs)
    return float(np.nanmean(diffs[mask])) if np.any(mask) else 0.0

def multicalib_mean_gap(proba, y, S_or_list, bins=10, min_support=1):
    proba = np.asarray(proba, float).reshape(-1)
    y     = np.asarray(y).reshape(-1).astype(float)
    if proba.size == 0 or y.size == 0: return 0.0
    if proba.shape[0] != y.shape[0]:
        raise ValueError(f"n mismatch: proba {proba.shape[0]} vs y {y.shape[0]}")

    S_bin = _to_bin_matrix(S_or_list)
    if S_bin.shape[0] != proba.shape[0]:
        raise ValueError(f"n mismatch: proba {proba.shape[0]} vs S {S_bin.shape[0]}")
    gid, G = _full_group_ids(S_bin)

    b = np.clip((proba * bins).astype(int), 0, bins - 1)
    key = gid * bins + b
    size = G * bins

    cnt = np.bincount(key, minlength=size)
    sy  = np.bincount(key, weights=y,      minlength=size)
    sp  = np.bincount(key, weights=proba,  minlength=size)

    with np.errstate(divide="ignore", invalid="ignore"):
        ey = np.where(cnt>0, sy/cnt, np.nan)
        ep = np.where(cnt>0, sp/cnt, np.nan)
        gap = np.abs(ey - ep)

    valid = (cnt >= min_support) & np.isfinite(gap)
    return float(np.nanmean(gap[valid])) if np.any(valid) else 0.0


# =====================================================================
# ============== ì—¬ê¸°ë¶€í„° Marginal(ì†ì„±ë³„) ë©”íŠ¸ë¦­ ì¶”ê°€ =================
# =====================================================================

def _families_from_S(S_or_list):
    """
    Së¥¼ ì†ì„± familyë¡œ ë¬¶ëŠ” í—¬í¼.
    - DataFrame: 'attr_value' í˜•íƒœì˜ ì»¬ëŸ¼ì€ 'attr'ë¥¼ familyë¡œ ë¬¶ìŒ.
                 '_'ê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ì»¬ëŸ¼ëª…ì´ ê³§ family(ì´ì§„).
    - list[dict] (image): ê° keyê°€ í•˜ë‚˜ì˜ ì´ì§„ ì†ì„± â†’ family[key] = [key]
    return: dict[str, list[str]]  (family -> list of columns/keys)
    """
    fam = {}
    if isinstance(S_or_list, list):
        keys = list(S_or_list[0].keys())
        for k in keys:
            fam[k] = [k]
    else:
        for c in S_or_list.columns:
            parts = str(c).split("_", 1)
            family = parts[0] if len(parts) > 1 else str(c)
            fam.setdefault(family, []).append(c)
    return fam

def _mask_from_key(S_or_list, key):
    if isinstance(S_or_list, list):
        return np.array([int(s[key]) for s in S_or_list])
    else:
        return S_or_list[key].values.astype(int)

def _spd_for_family(arr01, S_or_list, keys, min_support=1):
    """
    family ì•ˆì—ì„œì˜ SPD ì •ì˜:
      - keysê°€ 1ê°œ(ì´ì§„ ì†ì„±): |Pr(h=1|S=1) - Pr(h=1|S=0)|
      - keysê°€ ì—¬ëŸ¬ ê°œ(ì›-í•« ì¹´í…Œê³ ë¦¬): max_{i,j} |Pr(h=1|k_i) - Pr(h=1|k_j)|
    """
    if len(keys) == 1:
        m = _mask_from_key(S_or_list, keys[0]) == 1
        n1, n0 = m.sum(), (~m).sum()
        if n1 >= min_support and n0 >= min_support:
            return abs(float(arr01[m].mean()) - float(arr01[~m].mean()))
        return np.nan
    # multi-cat
    rates = []
    for k in keys:
        m = _mask_from_key(S_or_list, k) == 1
        if m.sum() >= min_support:
            rates.append(float(arr01[m].mean()))
    if len(rates) < 2:
        return np.nan
    return float(np.max(rates) - np.min(rates))

def _fpr_for_family(pred01, y, S_or_list, keys, min_support=1):
    """
    family ì•ˆì—ì„œì˜ FPR ì •ì˜(ìœ„ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ):
      - binary: |FPR(S=1) - FPR(S=0)|
      - multi-cat: max_{i,j} |FPR(k_i) - FPR(k_j)|
    """
    y = y.reshape(-1)
    pred01 = pred01.reshape(-1)
    mask_all = (y == 0)
    if mask_all.sum() < min_support:
        return np.nan

    def fpr_of(m):
        idx = mask_all & (m == 1)
        if idx.sum() < min_support:
            return None
        return float((pred01[idx] == 1).mean())

    if len(keys) == 1:
        m = _mask_from_key(S_or_list, keys[0])
        f1 = fpr_of(m)
        f0 = fpr_of(1 - m)
        if f1 is None or f0 is None:
            return np.nan
        return abs(f1 - f0)

    vals = []
    for k in keys:
        m = _mask_from_key(S_or_list, k)
        f = fpr_of(m)
        if f is not None:
            vals.append(f)
    if len(vals) < 2:
        return np.nan
    return float(np.max(vals) - np.min(vals))

def _mc_for_family(proba, y, S_or_list, keys, bins=10, min_support=1):
    """
    family ì•ˆì—ì„œì˜ multicalibration worst gap:
      - binary: max_b gap(S=1,b), gap(S=0,b) ì¤‘ ìµœëŒ€
      - multi-cat: max_{k in keys} max_b gap(k,b)
    """
    proba = proba.reshape(-1)
    y = y.reshape(-1)
    edges = np.linspace(0, 1, bins + 1)

    def max_gap_of_mask(m):
        worst = 0.0
        for i in range(bins):
            lo, hi = edges[i], edges[i + 1]
            idx = (m == 1) & (proba >= lo) & (proba < (hi if i < bins - 1 else proba.max() + 1e-12))
            if idx.sum() < min_support:
                continue
            worst = max(worst, abs(float(y[idx].mean()) - float(proba[idx].mean())))
        return worst if worst > 0 else None

    gaps = []
    if len(keys) == 1:
        m = _mask_from_key(S_or_list, keys[0])
        g1 = max_gap_of_mask(m)
        g0 = max_gap_of_mask(1 - m)
        for g in (g1, g0):
            if g is not None:
                gaps.append(g)
    else:
        for k in keys:
            m = _mask_from_key(S_or_list, k)
            g = max_gap_of_mask(m)
            if g is not None:
                gaps.append(g)

    if len(gaps) == 0:
        return np.nan
    return float(np.max(gaps))

# -------- Marginal SPD --------
def marginal_spd_worst(proba, S_or_list, thr=0.5, min_support=1, families=None):
    arr01 = (proba.reshape(-1) >= thr).astype(int)
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _spd_for_family(arr01, S_or_list, keys, min_support=min_support)
        if v == v:  # not NaN
            vals.append(v)
    return float(np.max(vals)) if len(vals) else 0.0

def marginal_spd_mean(proba, S_or_list, thr=0.5, min_support=1, families=None):
    arr01 = (proba.reshape(-1) >= thr).astype(int)
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _spd_for_family(arr01, S_or_list, keys, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.mean(vals)) if len(vals) else 0.0

# -------- Marginal FPR --------
def marginal_fpr_worst(pred, y, S_or_list, min_support=1, families=None):
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _fpr_for_family(pred, y, S_or_list, keys, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.max(vals)) if len(vals) else 0.0

def marginal_fpr_mean(pred, y, S_or_list, min_support=1, families=None):
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _fpr_for_family(pred, y, S_or_list, keys, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.mean(vals)) if len(vals) else 0.0

# -------- Marginal Multicalibration --------
def marginal_mc_worst(proba, y, S_or_list, bins=10, min_support=1, families=None):
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _mc_for_family(proba, y, S_or_list, keys, bins=bins, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.max(vals)) if len(vals) else 0.0

def marginal_mc_mean(proba, y, S_or_list, bins=10, min_support=1, families=None):
    fam = families or _families_from_S(S_or_list)
    vals = []
    for a, keys in fam.items():
        v = _mc_for_family(proba, y, S_or_list, keys, bins=bins, min_support=min_support)
        if v == v:
            vals.append(v)
    return float(np.mean(vals)) if len(vals) else 0.0